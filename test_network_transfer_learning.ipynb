{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='BARRA_R and ACCESS-S!')\n",
    "parser.add_argument('--args_test', type=int, default=0,\n",
    "                        help='testing parameters input')\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "                    help='Enables debug mode')\n",
    "parser.add_argument('--template', default='.',\n",
    "                    help='You can set various templates in option.py')\n",
    "\n",
    "# Hardware specifications\n",
    "parser.add_argument('--n_threads', type=int, default=0,\n",
    "                    help='number of threads for data loading')\n",
    "parser.add_argument('--cpu', action='store_true',\n",
    "                    help='use cpu only')\n",
    "parser.add_argument('--n_GPUs', type=int, default=1,\n",
    "                    help='number of GPUs')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed')\n",
    "\n",
    "# Data specifications\n",
    "parser.add_argument('--pr', type=bool, \n",
    "                default=True,\n",
    "                help='add-on pr?')\n",
    "\n",
    "parser.add_argument('--dem', type=bool, \n",
    "                default=False,\n",
    "                help='add-on dem?') \n",
    "parser.add_argument('--psl', type=bool, \n",
    "                default=False,\n",
    "                help='add-on psl?') \n",
    "parser.add_argument('--zg', type=bool, \n",
    "                default=False,\n",
    "                help='add-on zg?') \n",
    "parser.add_argument('--tasmax', type=bool, \n",
    "                default=False,\n",
    "                help='add-on tasmax?') \n",
    "parser.add_argument('--tasmin', type=bool, \n",
    "                default=False,\n",
    "                help='add-on tasmin?')\n",
    "\n",
    "parser.add_argument('--leading_time_we_use', type=int, \n",
    "                default=7,\n",
    "                help='add-on tasmin?')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--ensemble', type=int, \n",
    "                default=2,\n",
    "                help='total ensambles is 11') \n",
    "\n",
    "\n",
    "parser.add_argument('--channels', type=float, \n",
    "                    default=0,\n",
    "                    help='channel of data_input must') \n",
    "#[111.85, 155.875, -44.35, -9.975]\n",
    "parser.add_argument('--domain', type=list, \n",
    "                    default=[112.9, 154.25, -43.7425, -9.0],\n",
    "                    help='dataset directory')    \n",
    "\n",
    "\n",
    "parser.add_argument('--file_ACCESS_dir', type=str, \n",
    "                    default=\"F:/climate/access-s1/pr/daily/\",\n",
    "\n",
    "                    help='dataset directory')\n",
    "parser.add_argument('--file_BARRA_dir', type=str, \n",
    "                    default=\"C:/Users/JIA059/barra/\",\n",
    "                    help='dataset directory')\n",
    "\n",
    "parser.add_argument('--file_DEM_dir', type=str, \n",
    "                    default=\"../DEM/\",\n",
    "                    help='dataset directory')\n",
    "\n",
    "parser.add_argument('--nine2nine', type=bool, \n",
    "                    default=True,\n",
    "                    help='whether rainfall acculate from 9am to 9am')\n",
    "parser.add_argument('--date_minus_one', type=int, \n",
    "                    default=1,\n",
    "                    help='whether rainfall acculate from yesterday(1)/today(0) 9am to tody/tomorrow 9am')\n",
    "\n",
    "\n",
    "parser.add_argument('--dir_demo', type=str, default='../test',\n",
    "                    help='demo image directory')\n",
    "#     parser.add_argument('--data_train', type=str, default='BARRA_R',\n",
    "#                         help='train dataset name')\n",
    "#     parser.add_argument('--data_test', type=str, default='DIV2K',\n",
    "#                         help='test dataset name')\n",
    "parser.add_argument('--benchmark_noise', action='store_true',\n",
    "                    help='use noisy benchmark sets')\n",
    "parser.add_argument('--n_train', type=int, default=800,\n",
    "                    help='number of training set')\n",
    "parser.add_argument('--n_val', type=int, default=10,\n",
    "                    help='number of validation set')\n",
    "parser.add_argument('--offset_val', type=int, default=800,\n",
    "                    help='validation index offest')\n",
    "parser.add_argument('--ext', type=str, default='sep',\n",
    "                    help='dataset file extension')\n",
    "parser.add_argument('--scale', default='4',\n",
    "                    help='super resolution scale')\n",
    "parser.add_argument('--patch_size', type=int, default=192,\n",
    "                    help='output patch size')\n",
    "#??????????????????????????????????????????????????\n",
    "parser.add_argument('--rgb_range', type=int, default=400,\n",
    "                    help='maximum value of RGB')\n",
    "parser.add_argument('--n_colors', type=int, default=3,\n",
    "                    help='number of color channels to use')\n",
    "parser.add_argument('--noise', type=str, default='.',\n",
    "                    help='Gaussian noise std.')\n",
    "parser.add_argument('--chop', action='store_true',\n",
    "                    help='enable memory-efficient forward')\n",
    "\n",
    "# Model specifications\n",
    "parser.add_argument('--model', default='RCAN',\n",
    "                    help='model name')\n",
    "\n",
    "parser.add_argument('--act', type=str, default='relu',\n",
    "                    help='activation function')\n",
    "parser.add_argument('--pre_train', type=str, default='.',\n",
    "                    help='pre-trained model directory')\n",
    "parser.add_argument('--extend', type=str, default='.',\n",
    "                    help='pre-trained model directory')\n",
    "parser.add_argument('--n_resblocks', type=int, default=20,\n",
    "                    help='number of residual blocks')\n",
    "parser.add_argument('--n_feats', type=int, default=64,\n",
    "                    help='number of feature maps')\n",
    "parser.add_argument('--res_scale', type=float, default=1,\n",
    "                    help='residual scaling')\n",
    "parser.add_argument('--shift_mean', default=True,\n",
    "                    help='subtract pixel mean from the input')\n",
    "parser.add_argument('--precision', type=str, default='single',\n",
    "                    choices=('single', 'half','double'),\n",
    "                    help='FP precision for test (single | half)')\n",
    "\n",
    "# Training specifications\n",
    "\n",
    "parser.add_argument('--train_name', type=str, default='temp01',\n",
    "                    help='the trainning name of the set')\n",
    "parser.add_argument('--reset', action='store_true',\n",
    "                    help='reset the training')\n",
    "parser.add_argument('--test_every', type=int, default=1000,\n",
    "                    help='do test per every N batches')\n",
    "parser.add_argument('--epochs', type=int, default=300,\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--batch_size', type=int, default=8,\n",
    "                    help='input batch size for training')\n",
    "parser.add_argument('--split_batch', type=int, default=1,\n",
    "                    help='split the batch into smaller chunks')\n",
    "parser.add_argument('--self_ensemble', action='store_true',\n",
    "                    help='use self-ensemble method for test')\n",
    "parser.add_argument('--test_only', action='store_true',\n",
    "                    help='set this option to test the model')\n",
    "parser.add_argument('--gan_k', type=int, default=1,\n",
    "                    help='k value for adversarial loss')\n",
    "\n",
    "# Optimization specifications\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--lr_decay', type=int, default=200,\n",
    "                    help='learning rate decay per N epochs')\n",
    "parser.add_argument('--decay_type', type=str, default='step',\n",
    "                    help='learning rate decay type')\n",
    "parser.add_argument('--gamma', type=float, default=0.5,\n",
    "                    help='learning rate decay factor for step decay')\n",
    "parser.add_argument('--optimizer', default='ADAM',\n",
    "                    choices=('SGD', 'ADAM', 'RMSprop'),\n",
    "                    help='optimizer to use (SGD | ADAM | RMSprop)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='SGD momentum')\n",
    "parser.add_argument('--beta1', type=float, default=0.9,\n",
    "                    help='ADAM beta1')\n",
    "parser.add_argument('--beta2', type=float, default=0.999,\n",
    "                    help='ADAM beta2')\n",
    "parser.add_argument('--epsilon', type=float, default=1e-8,\n",
    "                    help='ADAM epsilon for numerical stability')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='weight decay')\n",
    "\n",
    "# Loss specifications\n",
    "parser.add_argument('--loss', type=str, default='1*L1',\n",
    "                    help='loss function configuration')\n",
    "parser.add_argument('--skip_threshold', type=float, default='1e6',\n",
    "                    help='skipping batch that has large error')\n",
    "\n",
    "# Log specifications\n",
    "parser.add_argument('--save', type=str, default='RCAN',\n",
    "                    help='file name to save')\n",
    "parser.add_argument('--load', type=str, default='.',\n",
    "                    help='file name to load')\n",
    "parser.add_argument('--resume', type=int, default=0,\n",
    "                    help='resume from specific checkpoint')\n",
    "parser.add_argument('--print_model', action='store_true',\n",
    "                    help='print model')\n",
    "parser.add_argument('--save_models', action='store_true',\n",
    "                    help='save all intermediate models')\n",
    "parser.add_argument('--print_every', type=int, default=100,\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save_results', action='store_true',\n",
    "                    help='save output results')\n",
    "\n",
    "# New options\n",
    "parser.add_argument('--n_resgroups', type=int, default=10,\n",
    "                    help='number of residual groups')\n",
    "parser.add_argument('--reduction', type=int, default=16,\n",
    "                    help='number of feature maps reduction')\n",
    "parser.add_argument('--testpath', type=str, default='../test/DIV2K_val_LR_our',\n",
    "                    help='dataset directory for testing')\n",
    "parser.add_argument('--testset', type=str, default='Set5',\n",
    "                    help='dataset name for testing')\n",
    "parser.add_argument('--degradation', type=str, default='BI',\n",
    "                    help='degradation model: BI, BD')\n",
    "# args = []\n",
    "# args = parser.parse_known_args()[0]\n",
    "import platform \n",
    "sys = platform.system()\n",
    "\n",
    "if sys == \"Windows\":\n",
    "    args = parser.parse_args(args=[])\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "#     template.set_template(args)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args.scale = list(map(lambda x: int(x), args.scale.split('+')))\n",
    "\n",
    "if args.epochs == 0:\n",
    "    args.epochs = 1e8\n",
    "\n",
    "for arg in vars(args):\n",
    "    if vars(args)[arg] == 'True':\n",
    "        vars(args)[arg] = True\n",
    "    elif vars(args)[arg] == 'False':\n",
    "        vars(args)[arg] = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.5114674452354135\n",
      "training statistics:\n",
      "  ------------------------------\n",
      "  trainning name  |  temp01\n",
      "  ------------------------------\n",
      "  trainning dtype  |  single\n",
      "  ------------------------------\n",
      "  num of channels |     3\n",
      "  ------------------------------\n",
      "  num of threads  |     0\n",
      "  ------------------------------\n",
      "  batch_size     |     8\n",
      "  ------------------------------\n",
      "  using cpu only？ |     1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "# from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v4\n",
    "\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import model\n",
    "from model import my_model\n",
    "import utility\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xarray as xr\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "import platform\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def write_log(log):\n",
    "    print(log)\n",
    "    my_log_file=open(\"./model/save/\"+args.train_name + '/train.txt', 'a')\n",
    "#     log=\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time())\n",
    "    my_log_file.write(log + '\\n')\n",
    "    my_log_file.close()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "init_date=date(1970, 1, 1)\n",
    "start_date=date(1990, 1, 2)\n",
    "end_date=date(2012,12,25) #if 929 is true we should substract 1 day    \n",
    "sys = platform.system()\n",
    "\n",
    "if sys == \"Windows\":\n",
    "    init_date=date(1970, 1, 1)\n",
    "    start_date=date(1990, 1, 2)\n",
    "    end_date=date(1990,12,15) #if 929 is true we should substract 1 day   \n",
    "#     args.file_ACCESS_dir=\"H:/climate/access-s1/\" \n",
    "#     args.file_BARRA_dir=\"D:/dataset/accum_prcp/\"\n",
    "    args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "    args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "    args.file_DEM_dir=\"../DEM/\"\n",
    "else:\n",
    "    args.file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "    args.file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "    # training_name=\"temp01\"\n",
    "    args.file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "\n",
    "args.channels=0\n",
    "args.zg=0\n",
    "args.psl=0\n",
    "args.tasmax=1\n",
    "args.tasmin=0\n",
    "args.dem=1\n",
    "\n",
    "if args.pr:\n",
    "    args.channels+=1\n",
    "if args.zg:\n",
    "    args.channels+=1\n",
    "if args.psl:\n",
    "    args.channels+=1\n",
    "if args.tasmax:\n",
    "    args.channels+=1\n",
    "if args.tasmin:\n",
    "    args.channels+=1\n",
    "print(args.dem)\n",
    "if args.dem:\n",
    "    args.channels+=1\n",
    "access_rgb_mean= 2.9067910245780248e-05*86400\n",
    "\n",
    "leading_time=217\n",
    "args.leading_time_we_use=7\n",
    "args.ensemble=2\n",
    "\n",
    "\n",
    "print(access_rgb_mean)\n",
    "\n",
    "print(\"training statistics:\")\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  trainning name  |  %s\"%args.train_name)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  trainning dtype  |  %s\"%args.precision)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  num of channels | %5d\"%args.channels)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  using cpu only？ | %5d\"%args.cpu)\n",
    "\n",
    "############################################################################################\n",
    "# args.cpu=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> BARRA_R & ACCESS_S1 loading\n",
      "=> from 1990/01/02 to 1990/12/15\n",
      "C:/Users/JIA059/barra/\n",
      "no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Dataset statistics:\n",
      "  ------------------------------\n",
      "  total |   587\n",
      "  ------------------------------\n",
      "  train |   469\n",
      "  ------------------------------\n",
      "  val   |   118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_transforms = transforms.Compose([\n",
    "#     transforms.Resize(IMG_SIZE),\n",
    "#     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(30),\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "])\n",
    "\n",
    "data_set=ACCESS_BARRA_v4(start_date,end_date,transform=train_transforms,args=args)\n",
    "train_data,val_data=random_split(data_set,[int(len(data_set)*0.8),len(data_set)-int(len(data_set)*0.8)])\n",
    "\n",
    "\n",
    "print(\"Dataset statistics:\")\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  total | %5d\"%len(data_set))\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  train | %5d\"%len(train_data))\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  val   | %5d\"%len(val_data))\n",
    "\n",
    "###################################################################################set a the dataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloders =DataLoader(train_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=False,\n",
    "                            num_workers=args.n_threads)\n",
    "val_dataloders =DataLoader(val_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=False,\n",
    "                          num_workers=args.n_threads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Making model...\n",
      "accesss-s1 mean (0.4690)\n",
      "Loading model from ./model/RCAN_BIX4.pt\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "args.cpu=True\n",
    "def prepare( l, volatile=False):\n",
    "    def _prepare(tensor):\n",
    "        if args.precision == 'half': tensor = tensor.half()\n",
    "        if args.precision == 'single': tensor = tensor.float()\n",
    "        return tensor.to(device)\n",
    "\n",
    "    return [_prepare(_l) for _l in l]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "checkpoint = utility.checkpoint(args)\n",
    "net = model.Model(args, checkpoint)\n",
    "net.load(\"./model/RCAN_BIX4.pt\", pre_train=\"./model/RCAN_BIX4.pt\", resume=args.resume, cpu=args.cpu)\n",
    "my_net=my_model.Modify_RCAN(net,args,checkpoint)\n",
    "# my_net = nn.DataParallel(my_net)\n",
    "args.lr=0.0001\n",
    "criterion = nn.L1Loss()\n",
    "optimizer_my = optim.SGD(my_net.parameters(), lr=args.lr, momentum=0.9)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer_my, step_size=7, gamma=0.1)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer_my, gamma=0.9)\n",
    "# torch.optim.lr_scheduler.MultiStepLR(optimizer_my, milestones=[20,80], gamma=0.1)\n",
    "\n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "#         # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#         net = nn.DataParallel(net)\n",
    "#     else:\n",
    "#         write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "\n",
    "\n",
    "# my_net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'model': my_net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': 0}\n",
    "torch.save(state, \"./model/save/temp01/first_\"+str(args.channels)+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x=torch.rand((8,64,79,94))\n",
    "args.channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_transfer.py  --n_threads 0  --batch_size 2 --n_resgroups 10 --n_resblocks 20 --patch_size 192 --pre_train ./model/RCAN_BIX4.pt --dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     y_my=my_net.body(x)\n",
    "!python3 train_transfer.py  --n_threads 0  --batch_size 2 --n_resgroups 10 --n_resblocks 20 --patch_size 192 --pre_train ./model/RCAN_BIX4.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     y=net.model.body(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y[0][0][0])\n",
    "# print(y_my[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #加载模型\n",
    "\n",
    "# # torch.save(model, \"./model/save/temp01/first.pth\")\n",
    "# torch.save({\n",
    "#             'epoch': 0,\n",
    "#             'model_state_dict': my_net.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer_my.state_dict(),\n",
    "#             }, \"./model/save/temp01/first.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for batch, (lr, hr,_,_) in enumerate(train_dataloders):\n",
    "#     lr=lr.expand((8,args.channels,79,94))\n",
    "#     lr, hr = prepare([lr, hr])\n",
    "#     with torch.no_grad():\n",
    "#         y=my_net(lr)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Modify_RCAN(nn.Module):\n",
    "#     def __init__(self,net,args,checkpoint):\n",
    "#         super().__init__()\n",
    "#         self.head=net.model.head\n",
    "# #         self.tail=net.model.tail\n",
    "\n",
    "#         modules_tail = [\n",
    "#         common.Upsampler(conv, scale, n_feats, act=False),\n",
    "# #             conv(n_feats, 1., kernel_size)\n",
    "#         conv(n_feats, args.channels, kernel_size)\n",
    "        \n",
    "#         ]\n",
    "\n",
    "#         self.body=net.model.body\n",
    "#         self.add_mean=net.model.add_mean\n",
    "#         self.sub_mean=net.model.sub_mean\n",
    "# #         self.body = nn.Sequential(\n",
    "# #                 net.model.head,\n",
    "# #                 net.model.body,\n",
    "# #                 net.model.tail\n",
    "# #         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(x.shape)\n",
    "#         x = self.sub_mean(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.head(x)\n",
    "#         res = self.body(x)\n",
    "#         print(res.shape)\n",
    "#         res += x\n",
    "# #         x = self.tail(res)\n",
    "#         print(x.shape)\n",
    "#         x = self.add_mean(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_net=Modify_RCAN(net,args,checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##########################################################################training\n",
    "\n",
    "# write_log(\"start\")\n",
    "# max_error=np.inf\n",
    "# for e in range(args.epochs):\n",
    "#     #train\n",
    "#     net.train()\n",
    "#     loss=0\n",
    "#     start=time.time()\n",
    "#     for batch, (lr, hr,_,_) in enumerate(train_dataloders):\n",
    "#         write_log(\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time()))\n",
    "#         start=time.time()\n",
    "# #             lr, hr = prepare([lr, hr])\n",
    "\n",
    "# #             optimizer_my.zero_grad()\n",
    "# #             with torch.set_grad_enabled(True):\n",
    "# #                 sr = net(lr, 0)\n",
    "# #                 running_loss =criterion(sr, hr)\n",
    "\n",
    "# #                 running_loss.backward()\n",
    "# #                 optimizer_my.step()\n",
    "# #             loss+=running_loss #.copy()?\n",
    "#         write_log(\"Train done,train time cost %f s\"%(start-time.time()))\n",
    "#         start=time.time()\n",
    "\n",
    "#     #validation\n",
    "#     net.eval()\n",
    "#     start=time.time()\n",
    "#     with torch.no_grad():\n",
    "#         eval_psnr=0\n",
    "#         eval_ssim=0\n",
    "# #             tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "#         for idx_img, (lr, hr,_,_) in enumerate(val_dataloders):\n",
    "#             lr, hr = prepare([lr, hr])\n",
    "#             sr = net(lr, 0)\n",
    "#             val_loss=criterion(sr, hr)\n",
    "#             for ssr,hhr in zip(sr,hr):\n",
    "#                 eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "#                 eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "\n",
    "#     write_log(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "#               e,\n",
    "#               time.time()-start,\n",
    "#               optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "#               loss.item()/len(train_data),\n",
    "#               val_loss\n",
    "#          ))\n",
    "# #         print(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "# #                   e,\n",
    "# #                   time.time()-start,\n",
    "# #                   optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "# #                   loss.item()/len(train_data),\n",
    "# #                   val_loss\n",
    "# #              ))\n",
    "#     if running_loss<max_error:\n",
    "#         max_error=running_loss\n",
    "# #         torch.save(net,train_loss\"_\"+str(e)+\".pkl\")\n",
    "#         if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "#             os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "#         write_log(\"saving\")    \n",
    "#         torch.save(net,\"./model/save/\"+args.train_name+\"/\"+str(e)+\".pkl\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from importlib import import_module\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, args, ckp):\n",
    "#         super(Model, self).__init__()\n",
    "#         print('Making model...')\n",
    "\n",
    "#         self.scale = args.scale\n",
    "#         self.idx_scale = 0\n",
    "#         self.self_ensemble = args.self_ensemble\n",
    "#         self.chop = args.chop\n",
    "#         self.precision = args.precision\n",
    "#         self.cpu = args.cpu\n",
    "#         self.device = torch.device('cpu' if args.cpu else 'cuda')\n",
    "#         self.n_GPUs = args.n_GPUs\n",
    "#         self.save_models = args.save_models\n",
    "\n",
    "#         module = import_module('model.' + args.model.lower())\n",
    "#         self.model = module.make_model(args).to(self.device)\n",
    "#         if args.precision == 'half': self.model.half()\n",
    "\n",
    "#         if not args.cpu and args.n_GPUs > 1:\n",
    "#             self.model = nn.DataParallel(self.model, range(args.n_GPUs))\n",
    "\n",
    "#         self.load(\n",
    "#             ckp.dir,\n",
    "#             pre_train=args.pre_train,\n",
    "#             resume=args.resume,\n",
    "#             cpu=args.cpu\n",
    "#         )\n",
    "#         if args.print_model: print(self.model)\n",
    "\n",
    "#     def forward(self, x, idx_scale):\n",
    "#         self.idx_scale = idx_scale\n",
    "#         target = self.get_model()\n",
    "#         if hasattr(target, 'set_scale'):\n",
    "#             target.set_scale(idx_scale)\n",
    "\n",
    "#         if self.self_ensemble and not self.training:\n",
    "#             if self.chop:\n",
    "#                 forward_function = self.forward_chop\n",
    "#             else:\n",
    "#                 forward_function = self.model.forward\n",
    "\n",
    "#             return self.forward_x8(x, forward_function)\n",
    "#         elif self.chop and not self.training:\n",
    "#             return self.forward_chop(x)\n",
    "#         else:\n",
    "#             return self.model(x)\n",
    "\n",
    "#     def get_model(self):\n",
    "#         if self.n_GPUs == 1:\n",
    "#             return self.model\n",
    "#         else:\n",
    "#             return self.model\n",
    "#             return self.model.module\n",
    "\n",
    "#     def state_dict(self, **kwargs):\n",
    "#         target = self.get_model()\n",
    "#         return target.state_dict(**kwargs)\n",
    "\n",
    "#     def save(self, apath, epoch, is_best=False):\n",
    "#         target = self.get_model()\n",
    "#         torch.save(\n",
    "#             target.state_dict(), \n",
    "#             os.path.join(apath, 'model', 'model_latest.pt')\n",
    "#         )\n",
    "#         if is_best:\n",
    "#             torch.save(\n",
    "#                 target.state_dict(),\n",
    "#                 os.path.join(apath, 'model', 'model_best.pt')\n",
    "#             )\n",
    "#         else:\n",
    "#             torch.save(\n",
    "#                 target.state_dict(),\n",
    "#                 os.path.join(apath, 'model', 'model_{}.pt'.format(epoch))\n",
    "#             )\n",
    "        \n",
    "#         if self.save_models:\n",
    "#             torch.save(\n",
    "#                 target.state_dict(),\n",
    "#                 os.path.join(apath, 'model', 'model_{}.pt'.format(epoch))\n",
    "#             )\n",
    "\n",
    "#     def load(self, apath, pre_train='.', resume=-1, cpu=False):\n",
    "#         if cpu:\n",
    "#             kwargs = {'map_location': lambda storage, loc: storage}\n",
    "#         else:\n",
    "#             kwargs = {}\n",
    "\n",
    "#         if resume == -1:\n",
    "#             self.get_model().load_state_dict(\n",
    "#                 torch.load(\n",
    "#                     os.path.join(apath, 'model', 'model_latest.pt'),\n",
    "#                     **kwargs\n",
    "#                 ),\n",
    "#                 strict=False\n",
    "#             )\n",
    "#         elif resume == 0:\n",
    "#             if pre_train != '.':\n",
    "#                 print('Loading model from {}'.format(pre_train))\n",
    "#                 self.get_model().load_state_dict(\n",
    "#                     torch.load(pre_train, **kwargs),\n",
    "#                     strict=False\n",
    "#                 )\n",
    "#         else:\n",
    "#             self.get_model().load_state_dict(\n",
    "#                 torch.load(\n",
    "#                     os.path.join(apath, 'model', 'model_{}.pt'.format(resume)),\n",
    "#                     **kwargs\n",
    "#                 ),\n",
    "#                 strict=False\n",
    "#             )\n",
    "#     # shave = 10, min_size=160000\n",
    "#     def forward_chop(self, x, shave=10, min_size=160000):\n",
    "#         scale = self.scale[self.idx_scale]\n",
    "#         n_GPUs = min(self.n_GPUs, 4)\n",
    "#         b, c, h, w = x.size()\n",
    "#         h_half, w_half = h // 2, w // 2\n",
    "#         h_size, w_size = h_half + shave, w_half + shave\n",
    "#         lr_list = [\n",
    "#             x[:, :, 0:h_size, 0:w_size],\n",
    "#             x[:, :, 0:h_size, (w - w_size):w],\n",
    "#             x[:, :, (h - h_size):h, 0:w_size],\n",
    "#             x[:, :, (h - h_size):h, (w - w_size):w]]\n",
    "\n",
    "#         if w_size * h_size < min_size:\n",
    "#             sr_list = []\n",
    "#             for i in range(0, 4, n_GPUs):\n",
    "#                 lr_batch = torch.cat(lr_list[i:(i + n_GPUs)], dim=0)\n",
    "#                 sr_batch = self.model(lr_batch)\n",
    "#                 sr_list.extend(sr_batch.chunk(n_GPUs, dim=0))\n",
    "#         else:\n",
    "#             sr_list = [\n",
    "#                 self.forward_chop(patch, shave=shave, min_size=min_size) \\\n",
    "#                 for patch in lr_list\n",
    "#             ]\n",
    "\n",
    "#         h, w = scale * h, scale * w\n",
    "#         h_half, w_half = scale * h_half, scale * w_half\n",
    "#         h_size, w_size = scale * h_size, scale * w_size\n",
    "#         shave *= scale\n",
    "\n",
    "#         output = x.new(b, c, h, w)\n",
    "#         output[:, :, 0:h_half, 0:w_half] \\\n",
    "#             = sr_list[0][:, :, 0:h_half, 0:w_half]\n",
    "#         output[:, :, 0:h_half, w_half:w] \\\n",
    "#             = sr_list[1][:, :, 0:h_half, (w_size - w + w_half):w_size]\n",
    "#         output[:, :, h_half:h, 0:w_half] \\\n",
    "#             = sr_list[2][:, :, (h_size - h + h_half):h_size, 0:w_half]\n",
    "#         output[:, :, h_half:h, w_half:w] \\\n",
    "#             = sr_list[3][:, :, (h_size - h + h_half):h_size, (w_size - w + w_half):w_size]\n",
    "\n",
    "#         return output\n",
    "\n",
    "#     def forward_x8(self, x, forward_function):\n",
    "#         def _transform(v, op):\n",
    "#             if self.precision != 'single': v = v.float()\n",
    "\n",
    "#             v2np = v.data.cpu().numpy()\n",
    "#             if op == 'v':\n",
    "#                 tfnp = v2np[:, :, :, ::-1].copy()\n",
    "#             elif op == 'h':\n",
    "#                 tfnp = v2np[:, :, ::-1, :].copy()\n",
    "#             elif op == 't':\n",
    "#                 tfnp = v2np.transpose((0, 1, 3, 2)).copy()\n",
    "\n",
    "#             ret = torch.Tensor(tfnp).to(self.device)\n",
    "#             if self.precision == 'half': ret = ret.half()\n",
    "\n",
    "#             return ret\n",
    "\n",
    "#         lr_list = [x]\n",
    "#         for tf in 'v', 'h', 't':\n",
    "#             lr_list.extend([_transform(t, tf) for t in lr_list])\n",
    "\n",
    "#         sr_list = [forward_function(aug) for aug in lr_list]\n",
    "#         for i in range(len(sr_list)):\n",
    "#             if i > 3:\n",
    "#                 sr_list[i] = _transform(sr_list[i], 't')\n",
    "#             if i % 4 > 1:\n",
    "#                 sr_list[i] = _transform(sr_list[i], 'h')\n",
    "#             if (i % 4) % 2 == 1:\n",
    "#                 sr_list[i] = _transform(sr_list[i], 'v')\n",
    "\n",
    "#         output_cat = torch.cat(sr_list, dim=0)\n",
    "#         output = output_cat.mean(dim=0, keepdim=True)\n",
    "\n",
    "#         return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
