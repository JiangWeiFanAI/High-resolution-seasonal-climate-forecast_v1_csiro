{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "# import args_parameter as args\n",
    "import torch,torchvision\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import time\n",
    "import xarray as xr\n",
    "\n",
    "class ACCESS_BARRA_v3(Dataset):\n",
    "    '''\n",
    "    scale is size(hr)=size(lr)*scale\n",
    "    version_3_documention: compare with ver1, I modify:\n",
    "    1. access file is created on getitem,the file list is access_date,barra,barra_date,time_leading\n",
    "      in order to read more data like zg etc. more easier, we change access_filepath to access_date\n",
    "\n",
    "    2. in ver.3, I extend the demention of the input data DEM.and change the domain to fit the size of dem. the shape also can be divided by 4\n",
    "   \n",
    "    '''\n",
    "    def __init__(self,start_date=date(1990, 1, 1),end_date=date(1990,12 , 31),regin=\"AUS\",transform=None,train=True,args=None):\n",
    "        print(\"=> BARRA_R & ACCESS_S1 loading\")\n",
    "        print(\"=> from \"+start_date.strftime(\"%Y/%m/%d\")+\" to \"+end_date.strftime(\"%Y/%m/%d\")+\"\")\n",
    "        self.file_BARRA_dir = args.file_BARRA_dir\n",
    "        self.file_ACCESS_dir = args.file_ACCESS_dir\n",
    "        self.args=args\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "        self.scale = args.scale[0]\n",
    "        self.regin = regin\n",
    "        self.leading_time=217\n",
    "        self.leading_time_we_use=args.leading_time_we_use\n",
    "\n",
    "        self.ensemble_access=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "        self.ensemble=[]\n",
    "        for i in range(args.ensemble):\n",
    "            self.ensemble.append(self.ensemble_access[i])\n",
    "                \n",
    "        self.dates = self.date_range(start_date, end_date)\n",
    "        \n",
    "        \n",
    "        self.filename_list=self.get_filename_with_time_order(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "        if not os.path.exists(args.file_ACCESS_dir+\"pr/daily/\"):\n",
    "            print(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "            print(\"no file or no permission\")\n",
    "        \n",
    "        \n",
    "        _,_,date_for_BARRA,time_leading=self.filename_list[0]\n",
    "        if not os.path.exists(\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/1990/01/accum_prcp-fc-spec-PT1H-BARRA_R-v1-19900109T0600Z.sub.nc\"):\n",
    "            print(self.file_BARRA_dir)\n",
    "            print(\"no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        data_high=dpt.read_barra_data_fc(self.file_BARRA_dir,date_for_BARRA,nine2nine=True)\n",
    "        data_exp=dpt.map_aust(data_high,domain=args.domain,xrarray=True)#,domain=domain)\n",
    "        self.lat=data_exp[\"lat\"]\n",
    "        self.lon=data_exp[\"lon\"]\n",
    "        self.shape=(81,108)\n",
    "        if self.args.dem:\n",
    "            self.dem_data=dpt.interp_tensor_2d(dpt.read_dem(args.file_DEM_dir+\"dem-9s1.tif\"),self.shape )\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filename_list)\n",
    "    \n",
    "\n",
    "    def date_range(self,start_date, end_date):\n",
    "        \"\"\"This function takes a start date and an end date as datetime date objects.\n",
    "        It returns a list of dates for each date in order starting at the first date and ending with the last date\"\"\"\n",
    "        return [start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    \n",
    "    def get_filename_with_no_time_order(self,rootdir):\n",
    "        '''get filename first and generate label '''\n",
    "        _files = []\n",
    "        list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "        for i in range(0,len(list)):\n",
    "            path = os.path.join(rootdir,list[i])\n",
    "            if os.path.isdir(path):\n",
    "                _files.extend(self.get_filename_with_no_time_order(path))\n",
    "            if os.path.isfile(path):\n",
    "                if path[-3:]==\".nc\":\n",
    "                    _files.append(path)\n",
    "        return _files\n",
    "    \n",
    "    def get_filename_with_time_order(self,rootdir):\n",
    "        '''get filename first and generate label ,one different w'''\n",
    "        _files = []\n",
    "        for en in self.ensemble:\n",
    "            for date in self.dates:\n",
    "                \n",
    "                    \n",
    "                \n",
    "                filename=\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "                access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "                if os.path.exists(access_path):\n",
    "                    for i in range(self.leading_time_we_use):\n",
    "                        if date==self.end_date and i==1:\n",
    "                            break\n",
    "                        path=[access_path]\n",
    "                        barra_date=date+timedelta(i)\n",
    "                        path.append(date)\n",
    "                        path.append(barra_date)\n",
    "                        path.append(i)\n",
    "                        _files.append(path)\n",
    "    \n",
    "    #最后去掉第一行，然后shuffle\n",
    "        if self.args.nine2nine and self.args.date_minus_one==1:\n",
    "            del _files[0]\n",
    "        return _files\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        from filename idx get id\n",
    "        return lr,hr\n",
    "        '''\n",
    "        t=time.time()\n",
    "        \n",
    "        #read_data filemame[idx]\n",
    "        access_filename_pr,access_date,date_for_BARRA,time_leading=self.filename_list[idx]\n",
    "#         print(type(date_for_BARRA))\n",
    "#         low_filename,high_filename,time_leading=self.filename_list[idx]\n",
    "\n",
    "        data_low=dpt.read_access_data(access_filename_pr,idx=time_leading)\n",
    "        lr_raw=dpt.map_aust(data_low,domain=self.args.domain,xrarray=False)\n",
    "        lr=np.expand_dims(dpt.interp_tensor_2d(lr_raw,self.shape),axis=2)\n",
    "        \n",
    "#         domain = [train_data.lon.data.min(), train_data.lon.data.max(), train_data.lat.data.min(), train_data.lat.data.max()]\n",
    "#         print(domain)\n",
    "\n",
    "        data_high=dpt.read_barra_data_fc(self.file_BARRA_dir,date_for_BARRA,nine2nine=True)\n",
    "        label=dpt.map_aust(data_high,domain=self.args.domain,xrarray=False)#,domain=domain)\n",
    "\n",
    "        \n",
    "        if self.args.zg:\n",
    "            access_filename_zg=self.args.file_ACCESS_dir+\"zg/daily/\"+en+\"/\"+\"da_zg_\"+access_date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "            data_zg=dpt.read_access_zg(access_filename_zg,idx=time_leading)\n",
    "            data_zg_aus=map_aust(data_zg,xrarray=False)\n",
    "            lr_zg=dpt.interp_tensor_3d(data_zg_aus,self.shape)\n",
    "            lr=np.concatenate(lr,np.expand_dims(lr_zg,axis=2),axis=2)\n",
    "        \n",
    "        if self.args.psl:\n",
    "            access_filename_psl=self.args.file_ACCESS_dir+\"psl/daily/\"+en+\"/\"+\"da_psl_\"+access_date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "            data_psl=dpt.read_access_data(access_filename_psl,idx=time_leading)\n",
    "            data_psl_aus=map_aust(data_psl,xrarray=False)\n",
    "            lr_psl=dpt.interp_tensor_2d(data_psl_aus,self.shape)\n",
    "            lr=np.concatenate(lr,np.expand_dims(lr_psl,axis=2),axis=2)\n",
    "        if self.args.tasmax:\n",
    "            access_filename_tasmax=self.args.file_ACCESS_dir+\"tasmax/daily/\"+en+\"/\"+\"da_tasmax_\"+access_date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "            data_tasmax=dpt.read_access_data(access_filename_tasmax,idx=time_leading)\n",
    "            data_tasmax_aus=map_aust(data_tasmax,xrarray=False)\n",
    "            lr_tasmax=dpt.interp_tensor_2d(data_tasmax_aus,self.shape)\n",
    "            lr=np.concatenate(lr,np.expand_dims(lr_tasmax,axis=2),axis=2)\n",
    "\n",
    "            \n",
    "        if self.args.tasmax:\n",
    "            access_filename_tasmin=self.args.file_ACCESS_dir+\"tasmin/daily/\"+en+\"/\"+\"da_tasmin_\"+access_date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "            data_tasmin=dpt.read_access_data(access_filename_tasmin,idx=time_leading)\n",
    "            data_tasmin_aus=map_aust(data_tasmin,xrarray=False)\n",
    "            lr_tasmin=dpt.interp_tensor_2d(data_tasmin_aus,self.shape)\n",
    "            lr=np.concatenate(lr,np.expand_dims(lr_tasmin,axis=2),axis=2)\n",
    "        if self.args.dem:\n",
    "            access_filename_tasmin=self.args.file_ACCESS_dir+\"tasmin/daily/\"+en+\"/\"+\"da_tasmin_\"+access_date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "            data_tasmin=dpt.read_access_data(access_filename_tasmin,idx=time_leading)\n",
    "            data_tasmin_aus=map_aust(data_tasmin,xrarray=False)\n",
    "            lr_tasmin=dpt.interp_tensor_2d(data_tasmin_aus,self.shape)\n",
    "            lr=np.concatenate(lr,np.expand_dims(lr_tasmin,axis=2),axis=2)\n",
    "\n",
    "            \n",
    "            \n",
    "        print(\"end loading one data,time cost %f\"%(time.time()-t))\n",
    "\n",
    "        if self.transform:#channel 数量需要整理！！\n",
    "            return self.transform(lr*86400),self.transform(label),torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "        else:\n",
    "            return lr*86400,label,torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "#         return np.reshape(train_data,(78,100,1))*86400,np.reshape(label,(312,400,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data stack\n",
    "\n",
    "dem_data=dpt.interp_tensor_2d(dpt.read_dem(\"../../../DEM/\"+\"dem-9s1.tif\"),(81,108))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_low=dpt.read_access_data(\"E:/climate/access-s1/pr/daily/e01/da_pr_19900101_e01.nc\",idx=0)\n",
    "lr_raw=dpt.map_aust(data_low,domain=[112.9, 154.00, -43.7425, -9.0],xrarray=False)\n",
    "lr_3=dpt.interp_tensor_2d(lr_raw,(81,108))\n",
    "lr_2=dpt.interp_tensor_2d(lr_raw,(81,108))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 108)\n",
      "(81, 108)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(lr_3.shape)\n",
    "print(lr_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 108, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.concatenate((np.expand_dims(lr_2,axis=2),np.expand_dims(dem_data,axis=2)),axis=2)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 108, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "b=np.concatenate((a,np.expand_dims(lr_3,axis=2)),axis=2)\n",
    "# b=np.stack((a,np.expand_dims(lr_3,axis=2)),axis=2)\n",
    "\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
