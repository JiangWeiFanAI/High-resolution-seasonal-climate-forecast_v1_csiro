{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     31
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "# from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v4\n",
    "from torch.utils.data import Dataset,random_split\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import model\n",
    "from model import my_model\n",
    "import utility\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xarray as xr\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "import platform\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def set_args():\n",
    "    parser = argparse.ArgumentParser(description='BARRA_R and ACCESS-S!')\n",
    "    parser.add_argument('--args_test', type=int, default=0,\n",
    "                            help='testing parameters input')\n",
    "    parser.add_argument('--debug', action='store_true',\n",
    "                        help='Enables debug mode')\n",
    "    parser.add_argument('--template', default='.',\n",
    "                        help='You can set various templates in option.py')\n",
    "\n",
    "    # Hardware specifications\n",
    "    parser.add_argument('--n_threads', type=int, default=0,\n",
    "                        help='number of threads for data loading')\n",
    "    parser.add_argument('--cpu', action='store_true',\n",
    "                        help='use cpu only')\n",
    "    parser.add_argument('--n_GPUs', type=int, default=1,\n",
    "                        help='number of GPUs')\n",
    "    parser.add_argument('--seed', type=int, default=1,\n",
    "                        help='random seed')\n",
    "\n",
    "    # Data specifications\n",
    "    parser.add_argument('--pr', type=bool, \n",
    "                    default=True,\n",
    "                    help='add-on pr?')\n",
    "\n",
    "    parser.add_argument('--dem', type=bool, \n",
    "                    default=True,\n",
    "                    help='add-on dem?') \n",
    "    parser.add_argument('--psl', type=bool, \n",
    "                    default=False,\n",
    "                    help='add-on psl?') \n",
    "    parser.add_argument('--zg', type=bool, \n",
    "                    default=False,\n",
    "                    help='add-on zg?') \n",
    "    parser.add_argument('--tasmax', type=bool, \n",
    "                    default=False,\n",
    "                    help='add-on tasmax?') \n",
    "    parser.add_argument('--tasmin', type=bool, \n",
    "                    default=False,\n",
    "                    help='add-on tasmin?')\n",
    "\n",
    "    parser.add_argument('--leading_time_we_use', type=int, \n",
    "                    default=7,\n",
    "                    help='add-on tasmin?')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    parser.add_argument('--ensemble', type=int, \n",
    "                    default=2,\n",
    "                    help='total ensambles is 11') \n",
    "\n",
    "\n",
    "    parser.add_argument('--channels', type=float, \n",
    "                        default=0,\n",
    "                        help='channel of data_input must') \n",
    "    #[111.85, 155.875, -44.35, -9.975]\n",
    "    parser.add_argument('--domain', type=list, \n",
    "                        default=[112.9, 154.25, -43.7425, -9.0],\n",
    "                        help='dataset directory')    \n",
    "\n",
    "\n",
    "    parser.add_argument('--file_ACCESS_dir', type=str, \n",
    "                        default=\"F:/climate/access-s1/pr/daily/\",\n",
    "\n",
    "                        help='dataset directory')\n",
    "    parser.add_argument('--file_BARRA_dir', type=str, \n",
    "                        default=\"C:/Users/JIA059/barra/\",\n",
    "                        help='dataset directory')\n",
    "\n",
    "    parser.add_argument('--file_DEM_dir', type=str, \n",
    "                        default=\"./DEM/\",\n",
    "                        help='dataset directory')\n",
    "\n",
    "    parser.add_argument('--nine2nine', type=bool, \n",
    "                        default=True,\n",
    "                        help='whether rainfall acculate from 9am to 9am')\n",
    "    parser.add_argument('--date_minus_one', type=int, \n",
    "                        default=1,\n",
    "                        help='whether rainfall acculate from yesterday(1)/today(0) 9am to tody/tomorrow 9am')\n",
    "\n",
    "\n",
    "    parser.add_argument('--dir_demo', type=str, default='../test',\n",
    "                        help='demo image directory')\n",
    "    #     parser.add_argument('--data_train', type=str, default='BARRA_R',\n",
    "    #                         help='train dataset name')\n",
    "    #     parser.add_argument('--data_test', type=str, default='DIV2K',\n",
    "    #                         help='test dataset name')\n",
    "    parser.add_argument('--benchmark_noise', action='store_true',\n",
    "                        help='use noisy benchmark sets')\n",
    "    parser.add_argument('--n_train', type=int, default=800,\n",
    "                        help='number of training set')\n",
    "    parser.add_argument('--n_val', type=int, default=10,\n",
    "                        help='number of validation set')\n",
    "    parser.add_argument('--offset_val', type=int, default=800,\n",
    "                        help='validation index offest')\n",
    "    parser.add_argument('--ext', type=str, default='sep',\n",
    "                        help='dataset file extension')\n",
    "    parser.add_argument('--scale', default='4',\n",
    "                        help='super resolution scale')\n",
    "    parser.add_argument('--patch_size', type=int, default=96,\n",
    "                        help='output patch size')\n",
    "    #??????????????????????????????????????????????????\n",
    "    parser.add_argument('--rgb_range', type=int, default=400,\n",
    "                        help='maximum value of RGB')\n",
    "    parser.add_argument('--n_colors', type=int, default=1,\n",
    "                        help='number of color channels to use')\n",
    "    parser.add_argument('--noise', type=str, default='.',\n",
    "                        help='Gaussian noise std.')\n",
    "    parser.add_argument('--chop', action='store_true',\n",
    "                        help='enable memory-efficient forward')\n",
    "\n",
    "    # Model specifications\n",
    "    parser.add_argument('--model', default='RCAN',\n",
    "                        help='model name')\n",
    "\n",
    "    parser.add_argument('--act', type=str, default='relu',\n",
    "                        help='activation function')\n",
    "    parser.add_argument('--pre_train', type=str, default='.',\n",
    "                        help='pre-trained model directory')\n",
    "    parser.add_argument('--extend', type=str, default='.',\n",
    "                        help='pre-trained model directory')\n",
    "    parser.add_argument('--n_resblocks', type=int, default=16,\n",
    "                        help='number of residual blocks')\n",
    "    parser.add_argument('--n_feats', type=int, default=64,\n",
    "                        help='number of feature maps')\n",
    "    parser.add_argument('--res_scale', type=float, default=1,\n",
    "                        help='residual scaling')\n",
    "    parser.add_argument('--shift_mean', default=True,\n",
    "                        help='subtract pixel mean from the input')\n",
    "    parser.add_argument('--precision', type=str, default='single',\n",
    "                        choices=('single', 'half'),\n",
    "                        help='FP precision for test (single | half)')\n",
    "\n",
    "    # Training specifications\n",
    "\n",
    "    parser.add_argument('--train_name', type=str, default='temp01',\n",
    "                        help='the trainning name of the set')\n",
    "    parser.add_argument('--reset', action='store_true',\n",
    "                        help='reset the training')\n",
    "    parser.add_argument('--test_every', type=int, default=1000,\n",
    "                        help='do test per every N batches')\n",
    "    parser.add_argument('--epochs', type=int, default=300,\n",
    "                        help='number of epochs to train')\n",
    "    parser.add_argument('--batch_size', type=int, default=8,\n",
    "                        help='input batch size for training')\n",
    "    parser.add_argument('--split_batch', type=int, default=1,\n",
    "                        help='split the batch into smaller chunks')\n",
    "    parser.add_argument('--self_ensemble', action='store_true',\n",
    "                        help='use self-ensemble method for test')\n",
    "    parser.add_argument('--test_only', action='store_true',\n",
    "                        help='set this option to test the model')\n",
    "    parser.add_argument('--gan_k', type=int, default=1,\n",
    "                        help='k value for adversarial loss')\n",
    "\n",
    "    # Optimization specifications\n",
    "    parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--lr_decay', type=int, default=200,\n",
    "                        help='learning rate decay per N epochs')\n",
    "    parser.add_argument('--decay_type', type=str, default='step',\n",
    "                        help='learning rate decay type')\n",
    "    parser.add_argument('--gamma', type=float, default=0.5,\n",
    "                        help='learning rate decay factor for step decay')\n",
    "    parser.add_argument('--optimizer', default='ADAM',\n",
    "                        choices=('SGD', 'ADAM', 'RMSprop'),\n",
    "                        help='optimizer to use (SGD | ADAM | RMSprop)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='SGD momentum')\n",
    "    parser.add_argument('--beta1', type=float, default=0.9,\n",
    "                        help='ADAM beta1')\n",
    "    parser.add_argument('--beta2', type=float, default=0.999,\n",
    "                        help='ADAM beta2')\n",
    "    parser.add_argument('--epsilon', type=float, default=1e-8,\n",
    "                        help='ADAM epsilon for numerical stability')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                        help='weight decay')\n",
    "\n",
    "    # Loss specifications\n",
    "    parser.add_argument('--loss', type=str, default='1*L1',\n",
    "                        help='loss function configuration')\n",
    "    parser.add_argument('--skip_threshold', type=float, default='1e6',\n",
    "                        help='skipping batch that has large error')\n",
    "\n",
    "    # Log specifications\n",
    "    parser.add_argument('--save', type=str, default='RCAN',\n",
    "                        help='file name to save')\n",
    "    parser.add_argument('--load', type=str, default='.',\n",
    "                        help='file name to load')\n",
    "    parser.add_argument('--resume', type=int, default=0,\n",
    "                        help='resume from specific checkpoint')\n",
    "    parser.add_argument('--print_model', action='store_true',\n",
    "                        help='print model')\n",
    "    parser.add_argument('--save_models', action='store_true',\n",
    "                        help='save all intermediate models')\n",
    "    parser.add_argument('--print_every', type=int, default=100,\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save_results', action='store_true',\n",
    "                        help='save output results')\n",
    "\n",
    "    # New options\n",
    "    parser.add_argument('--n_resgroups', type=int, default=10,\n",
    "                        help='number of residual groups')\n",
    "    parser.add_argument('--reduction', type=int, default=16,\n",
    "                        help='number of feature maps reduction')\n",
    "    parser.add_argument('--testpath', type=str, default='../test/DIV2K_val_LR_our',\n",
    "                        help='dataset directory for testing')\n",
    "    parser.add_argument('--testset', type=str, default='Set5',\n",
    "                        help='dataset name for testing')\n",
    "    parser.add_argument('--degradation', type=str, default='BI',\n",
    "                        help='degradation model: BI, BD')\n",
    "    # args = []\n",
    "    # args = parser.parse_known_args()[0]\n",
    "    # import platform \n",
    "    # sys = platform.system()\n",
    "    # if sys == \"Windows\":\n",
    "    #     args = parser.parse_args(args=[])\n",
    "    # else:\n",
    "    #     args = parser.parse_args()\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "\n",
    "    #     template.set_template(args)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    args.scale = list(map(lambda x: int(x), args.scale.split('+')))\n",
    "\n",
    "    if args.epochs == 0:\n",
    "        args.epochs = 1e8\n",
    "\n",
    "    for arg in vars(args):\n",
    "        if vars(args)[arg] == 'True':\n",
    "            vars(args)[arg] = True\n",
    "        elif vars(args)[arg] == 'False':\n",
    "            vars(args)[arg] = False\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args=set_args()\n",
    "\n",
    "args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "args.file_DEM_dir=\"../DEM/\"\n",
    "\n",
    "init_date=date(1970, 1, 1)\n",
    "start_date=date(1990, 1, 2)\n",
    "end_date=date(1990,12,10)\n",
    "args.channels=0\n",
    "args.dem=True\n",
    "args.psl=True\n",
    "args.zg=True\n",
    "args.tasmax=True\n",
    "args.tasmin=True\n",
    "\n",
    "\n",
    "\n",
    "if args.pr:\n",
    "    args.channels+=1\n",
    "if args.zg:\n",
    "    args.channels+=22\n",
    "if args.psl:\n",
    "    args.channels+=1\n",
    "if args.tasmax:\n",
    "    args.channels+=1\n",
    "if args.tasmin:\n",
    "    args.channels+=1\n",
    "if args.dem:\n",
    "    args.channels+=1\n",
    "access_rgb_mean= 2.9067910245780248e-05*86400\n",
    "\n",
    "leading_time=217\n",
    "args.leading_time_we_use=7\n",
    "args.ensemble=2\n",
    "\n",
    "class ACCESS_BARRA_v4(Dataset):\n",
    "    '''\n",
    "    scale is size(hr)=size(lr)*scale\n",
    "    version_3_documention: compare with ver1, I modify:\n",
    "    1. access file is created on getitem,the file list is access_date,barra,barra_date,time_leading\n",
    "      in order to read more data like zg etc. more easier, we change access_filepath to access_date\n",
    "\n",
    "    2. in ver., norm the every inputs \n",
    "   \n",
    "    '''\n",
    "    def __init__(self,start_date=date(1990, 1, 1),end_date=date(1990,12 , 31),regin=\"AUS\",transform=None,train=True,args=None):\n",
    "        print(\"=> BARRA_R & ACCESS_S1 loading\")\n",
    "        print(\"=> from \"+start_date.strftime(\"%Y/%m/%d\")+\" to \"+end_date.strftime(\"%Y/%m/%d\")+\"\")\n",
    "        self.file_BARRA_dir = args.file_BARRA_dir\n",
    "        self.file_ACCESS_dir = args.file_ACCESS_dir\n",
    "        self.args=args\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "        self.scale = args.scale[0]\n",
    "        self.regin = regin\n",
    "        self.leading_time=217\n",
    "        self.leading_time_we_use=args.leading_time_we_use\n",
    "\n",
    "        self.ensemble_access=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "        self.ensemble=[]\n",
    "        for i in range(args.ensemble):\n",
    "            self.ensemble.append(self.ensemble_access[i])\n",
    "                \n",
    "        self.dates = self.date_range(start_date, end_date)\n",
    "        \n",
    "        \n",
    "        self.filename_list=self.get_filename_with_time_order(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "        if not os.path.exists(args.file_ACCESS_dir+\"pr/daily/\"):\n",
    "            print(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "            print(\"no file or no permission\")\n",
    "        \n",
    "        \n",
    "        _,_,_,date_for_BARRA,time_leading=self.filename_list[0]\n",
    "        if not os.path.exists(\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/1990/01/accum_prcp-fc-spec-PT1H-BARRA_R-v1-19900109T0600Z.sub.nc\"):\n",
    "            print(self.file_BARRA_dir)\n",
    "            print(\"no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        data_high=dpt.read_barra_data_fc(self.file_BARRA_dir,date_for_BARRA,nine2nine=True)\n",
    "        data_exp=dpt.map_aust(data_high,domain=args.domain,xrarray=True)#,domain=domain)\n",
    "        self.lat=data_exp[\"lat\"]\n",
    "        self.lon=data_exp[\"lon\"]\n",
    "        self.shape=(79,94)\n",
    "        if self.args.dem:\n",
    "            data_dem=dpt.add_lat_lon( dpt.read_dem(args.file_DEM_dir+\"dem-9s1.tif\"))\n",
    "            self.dem_data=dpt.interp_tensor_2d(dpt.map_aust_old(data_dem,xrarray=False) ,self.shape )\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filename_list)\n",
    "    \n",
    "\n",
    "    def date_range(self,start_date, end_date):\n",
    "        \"\"\"This function takes a start date and an end date as datetime date objects.\n",
    "        It returns a list of dates for each date in order starting at the first date and ending with the last date\"\"\"\n",
    "        return [start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    \n",
    "    def get_filename_with_no_time_order(self,rootdir):\n",
    "        '''get filename first and generate label '''\n",
    "        _files = []\n",
    "        list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "        for i in range(0,len(list)):\n",
    "            path = os.path.join(rootdir,list[i])\n",
    "            if os.path.isdir(path):\n",
    "                _files.extend(self.get_filename_with_no_time_order(path))\n",
    "            if os.path.isfile(path):\n",
    "                if path[-3:]==\".nc\":\n",
    "                    _files.append(path)\n",
    "        return _files\n",
    "    \n",
    "    def get_filename_with_time_order(self,rootdir):\n",
    "        '''get filename first and generate label ,one different w'''\n",
    "        _files = []\n",
    "        for en in self.ensemble:\n",
    "            for date in self.dates:\n",
    "                \n",
    "                    \n",
    "                \n",
    "#                 filename=\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"cd\n",
    "                access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "                if os.path.exists(access_path):\n",
    "                    for i in range(self.leading_time_we_use):\n",
    "                        if date==self.end_date and i==1:\n",
    "                            break\n",
    "                        path=[access_path]\n",
    "                        path.append(en)\n",
    "                        barra_date=date+timedelta(i)\n",
    "                        path.append(date)\n",
    "                        path.append(barra_date)\n",
    "                        path.append(i)\n",
    "                        _files.append(path)\n",
    "    \n",
    "    #最后去掉第一行，然后shuffle\n",
    "        if self.args.nine2nine and self.args.date_minus_one==1:\n",
    "            del _files[0]\n",
    "        return _files\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        from filename idx get id\n",
    "        return lr,hr\n",
    "        '''\n",
    "        t=time.time()\n",
    "        \n",
    "        #read_data filemame[idx]\n",
    "        access_filename_pr,en,access_date,date_for_BARRA,time_leading=self.filename_list[idx]\n",
    "#         print(type(date_for_BARRA))\n",
    "#         low_filename,high_filename,time_leading=self.filename_list[idx]\n",
    "\n",
    "        lr=dpt.read_access_data(access_filename_pr,idx=time_leading).data[82:144,134:188]*86400\n",
    "#         lr=dpt.map_aust(lr,domain=self.args.domain,xrarray=False)\n",
    "#         lr=np.expand_dims(dpt.interp_tensor_2d(lr,self.shape),axis=2)\n",
    "\n",
    "        lr=dpt.interp_tensor_2d(lr,self.shape)\n",
    "\n",
    "#         lr.dtype=\"float32\"\n",
    "\n",
    "\n",
    "        data_high=dpt.read_barra_data_fc(self.file_BARRA_dir,date_for_BARRA,nine2nine=True)\n",
    "        label=dpt.map_aust(data_high,domain=self.args.domain,xrarray=False)#,domain=domain)\n",
    "\n",
    "        if self.args.zg:\n",
    "            access_filename_zg=self.args.file_ACCESS_dir+\"zg/daily/\"+en+\"/\"+\"da_zg_\"+access_date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "            lr_zg=dpt.read_access_zg(access_filename_zg,idx=time_leading).data[:][83:145,135:188]\n",
    "            lr_zg=dpt.interp_tensor_3d(lr_zg,self.shape)\n",
    "        \n",
    "        if self.args.psl:\n",
    "            access_filename_psl=self.args.file_ACCESS_dir+\"psl/daily/\"+en+\"/\"+\"da_psl_\"+access_date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "            lr_psl=dpt.read_access_data(access_filename_psl,var_name=\"psl\",idx=time_leading).data[82:144,134:188]\n",
    "            lr_psl=dpt.interp_tensor_2d(lr_psl,self.shape)\n",
    "\n",
    "        if self.args.tasmax:\n",
    "            access_filename_tasmax=self.args.file_ACCESS_dir+\"tasmax/daily/\"+en+\"/\"+\"da_tasmax_\"+access_date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "            lr_tasmax=dpt.read_access_data(access_filename_tasmax,var_name=\"tasmax\",idx=time_leading).data[82:144,134:188]\n",
    "            lr_tasmax=dpt.interp_tensor_2d(lr_tasmax,self.shape)\n",
    "            \n",
    "        if self.args.tasmin:\n",
    "            access_filename_tasmin=self.args.file_ACCESS_dir+\"tasmin/daily/\"+en+\"/\"+\"da_tasmin_\"+access_date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "            lr_tasmin=dpt.read_access_data(access_filename_tasmin,var_name=\"tasmin\",idx=time_leading).data[82:144,134:188]\n",
    "            lr_tasmin=dpt.interp_tensor_2d(lr_tasmin,self.shape)\n",
    "\n",
    "            \n",
    "#         if self.args.dem:\n",
    "# #             print(\"add dem data\")\n",
    "#             lr=np.concatenate((lr,np.expand_dims(self.dem_data,axis=2)),axis=2)\n",
    "\n",
    "            \n",
    "#         print(\"end loading one data,time cost %f\"%(time.time()-t))\n",
    "\n",
    "\n",
    "        if self.transform:#channel 数量需要整理！！\n",
    "            if self.args.channels==6:\n",
    "                return self.transform(lr),self.transform(self.dem_data),self.transform(lr_psl),self.transform(lr_zg),self.transform(lr_tasmax),self.transform(lr_tasmin),self.transform(label),torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "            if self.args.channels==1:\n",
    "#                 return self.transform(lr),self.transform(label),torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "                return self.transform(lr),None,self.transform(lr_psl),self.transform(lr_zg),self.transform(lr_tasmax),self.transform(lr_tasmin),self.transform(label),torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "\n",
    "            \n",
    "            if self.args.channels==2:\n",
    "                return self.transform(lr),self.transform(self.dem_data),self.transform(label),torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "\n",
    "        else:\n",
    "            return lr,label,torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "#         return np.reshape(train_data,(78,100,1))*86400,np.reshape(label,(312,400,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 23, 4], 654654, 654654]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def a(l):\n",
    "\n",
    "    return [_l for _l in l if _l is not None]\n",
    "\n",
    "l=[[1,23,4,],None,654654,654654]\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training statistics:\n",
      "  ------------------------------\n",
      "  trainning name  |  temp01\n",
      "  ------------------------------\n",
      "  num of channels |    27\n",
      "  ------------------------------\n",
      "  num of threads  |     0\n",
      "  ------------------------------\n",
      "  batch_size     |     8\n",
      "  ------------------------------\n",
      "  using cpu only？ |     0\n"
     ]
    }
   ],
   "source": [
    "    print(\"training statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  trainning name  |  %s\"%args.train_name)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of channels | %5d\"%args.channels)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  using cpu only？ | %5d\"%args.cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> BARRA_R & ACCESS_S1 loading\n",
      "=> from 1990/01/02 to 1990/01/10\n",
      "C:/Users/JIA059/barra/\n",
      "no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset=ACCESS_BARRA_v4(\n",
    "    start_date=date(1990, 1, 2),\n",
    "    end_date=date(1990, 1, 10),\n",
    "    transform=train_transforms,\n",
    "    args=args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloders =DataLoader(dataset,\n",
    "                                            batch_size=2,\n",
    "                                            shuffle=False,\n",
    "                                num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 79, 94])\n",
      "torch.Size([1, 79, 94])\n",
      "torch.Size([1, 79, 94])\n",
      "torch.Size([512, 79, 94])\n",
      "torch.Size([1, 79, 94])\n",
      "torch.Size([1, 79, 94])\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(a[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr\n"
     ]
    }
   ],
   "source": [
    "for batch, (pr,dem,psl,zg,tasmax,tasmain, hr,_,_) in enumerate(train_dataloders):\n",
    "    print(\"pr\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data stack\n",
    "\n",
    "# dem_data=dpt.interp_tensor_2d(dpt.read_dem(\"../../../DEM/\"+\"dem-9s1.tif\"),(79,94))\n",
    "\n",
    "data_dem=dpt.add_lat_lon( dpt.read_dem(\"../../../DEM/\"+\"dem-9s1.tif\"))\n",
    "self.dem_data=dpt.interp_tensor_2d(dpt.map_aust_old(data_dem,xrarray=False) ,self.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_low=dpt.read_access_data(\"E:/climate/access-s1/pr/daily/e01/da_pr_19900101_e01.nc\",idx=0)\n",
    "lr_raw=dpt.map_aust(data_low,domain=[112.9, 154.00, -43.7425, -9.0],xrarray=False)\n",
    "lr_3=dpt.interp_tensor_2d(lr_raw,(81,108))\n",
    "lr_2=dpt.interp_tensor_2d(lr_raw,(81,108))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(lr_3.shape)\n",
    "print(lr_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.concatenate((np.expand_dims(lr_2,axis=2),np.expand_dims(dem_data,axis=2)),axis=2)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b=np.concatenate((a,np.expand_dims(lr_3,axis=2)),axis=2)\n",
    "# b=np.stack((a,np.expand_dims(lr_3,axis=2)),axis=2)\n",
    "\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
