{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5114674452354135\n",
      "training statistics:\n",
      "  ------------------------------\n",
      "  trainning name  |  temp01\n",
      "  ------------------------------\n",
      "  num of channels |     1\n",
      "  ------------------------------\n",
      "  num of threads  |     2\n",
      "  ------------------------------\n",
      "  batch_size      |     4\n"
     ]
    }
   ],
   "source": [
    "#for debuging on my computer\n",
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v3,ACCESS_BARRA_v2\n",
    "\n",
    "\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import time\n",
    "import model\n",
    "import utility \n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xarray as xr\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "import platform \n",
    "sys = platform.system()\n",
    "if sys == \"Windows\":\n",
    "    args.file_ACCESS_dir=\"H:/climate/access-s1/\"\n",
    "    args.file_BARRA_dir=\"D:/dataset/accum_prcp/\"\n",
    "else:\n",
    "    args.file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "    args.file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "    # training_name=\"temp01\"\n",
    "    args.file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "\n",
    "args.n_threads=2\n",
    "args.channels=1\n",
    "args.batch_size=4\n",
    "\n",
    "# ensemble=['e01','e02']\n",
    "args.ensemble=2\n",
    "access_rgb_mean= 2.9067910245780248e-05*86400\n",
    "\n",
    "leading_time=217\n",
    "args.leading_time_we_use=7\n",
    "# args.lr=0.001\n",
    "\n",
    "init_date=date(1970, 1, 1)\n",
    "start_date=date(1990, 1, 2)\n",
    "end_date=date(1990,12,25) #if 929 is true we should substract 1 day\n",
    "\n",
    "print(access_rgb_mean)\n",
    "\n",
    "print(\"training statistics:\")\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  trainning name  |  %s\"%args.train_name)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  num of channels | %5d\"%args.channels)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  batch_size      | %5d\"%args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'epoche: 1,time cost 1.000000 s, lr: 1.000000, train_loss: 1.000000,validation loss:1.000000 '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint = utility.checkpoint(args)\n",
    "# log=\"Dataset statisttyiutyuituiics:\"\n",
    "# checkpoint.my_write_log(log,True)\n",
    "# c=0\n",
    "# a=\"123123 %d\"%c\n",
    "e,\n",
    "time.time()-start,\n",
    "optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "loss.item()/len(train_data),=0\n",
    "val_loss=0\n",
    "a=\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "                  e,\n",
    "                  time.time()-start,\n",
    "                  optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "                  loss.item()/len(train_data),\n",
    "                  val_loss\n",
    "             )\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> BARRA_R & ACCESS_S1 loading\n",
      "=> from 1990/01/02 to 1990/12/25\n",
      "D:/dataset/accum_prcp/\n",
      "no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ae9564b85bc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m ])\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdata_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mACCESS_BARRA_v3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_transforms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Climate_change\\High-resolution-seasonal-climate-forecast_v1_csiro\\PrepareData.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, start_date, end_date, regin, transform, train, args)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdem\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[0mdata_dem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_lat_lon\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_dem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_DEM_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"dem-9s1.tif\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdem_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterp_tensor_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_aust_old\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxrarray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Climate_change\\High-resolution-seasonal-climate-forecast_v1_csiro\\data_processing_tool.py\u001b[0m in \u001b[0;36mmap_aust_old\u001b[1;34m(data, lat, lon, domain, xrarray)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlon\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlon\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlat\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m     \u001b[0mda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m     \u001b[0mllons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mllats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 将维度按照 x,y 横向竖向\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"<class 'xarray.core.dataarray.DataArray'>\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mxrarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_transforms = transforms.Compose([\n",
    "#     transforms.Resize(IMG_SIZE),\n",
    "#     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(30),\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "])\n",
    "\n",
    "data_set=ACCESS_BARRA_v3(start_date,end_date,transform=train_transforms,args=args)\n",
    "train_data,val_data=random_split(data_set,[int(len(data_set)*0.8),len(data_set)-int(len(data_set)*0.8)])\n",
    "\n",
    "\n",
    "print(\"Dataset statistics:\")\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  total | %5d\"%len(data_set))\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  train | %5d\"%len(train_data))\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  val   | %5d\"%len(val_data))\n",
    "data_set[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##set a the dataLoader\n",
    "train_dataloders =DataLoader(train_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=False,\n",
    "                            num_workers=args.n_threads)\n",
    "val_dataloders =DataLoader(val_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=False,\n",
    "                          num_workers=args.n_threads)\n",
    "##\n",
    "\n",
    "# args.cpu=False\n",
    "def prepare( l, volatile=False):\n",
    "    device = torch.device('cpu' if args.cpu else 'cuda')\n",
    "    def _prepare(tensor):\n",
    "        if args.precision == 'half': tensor = tensor.half()\n",
    "        return tensor.to(device)\n",
    "\n",
    "    return [_prepare(_l) for _l in l]\n",
    "\n",
    "checkpoint = utility.checkpoint(args)\n",
    "net = model.Model(args, checkpoint).double()\n",
    "args.lr=0.001\n",
    "criterion = nn.L1Loss()\n",
    "optimizer_my = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer_my, step_size=7, gamma=0.1)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer_my, gamma=0.9)\n",
    "# torch.optim.lr_scheduler.MultiStepLR(optimizer_my, milestones=[20,80], gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#training\n",
    "max_error=np.inf\n",
    "# train_loss=\n",
    "# print(\"batch size %d\"%args.batch_size)\n",
    "# print(\"batch size %d\"%args.batch_size)\n",
    "\n",
    "for e in range(args.epochs):\n",
    "    #train\n",
    "    print(\"start Training for %d\"%e)\n",
    "#     net.train()\n",
    "    loss=0\n",
    "    start=time.time()\n",
    "    print(\"start Training for start loading data\")\n",
    "\n",
    "    for batch, (lr, hr,_,_) in enumerate(train_dataloders):\n",
    "        print(\"start first batch train\")\n",
    "        if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "            os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "        f = open(\"./model/save/\"+args.train_name+\"/\"+str(batch)+\".txt\",'w')\n",
    "        f.close()\n",
    "        lr, hr = prepare([lr, hr])\n",
    "#         optimizer_my.zero_grad()\n",
    "#         with torch.set_grad_enabled(True):\n",
    "#             sr = net(lr, 0)\n",
    "# #         error = criterion(sr[:,:,:,0:403], hr)\n",
    "#             running_loss =criterion(sr, hr)\n",
    "#             loss+=running_loss \n",
    "#         running_loss.backward()\n",
    "#         optimizer_my.step()\n",
    "        print(\"end first batch train\")\n",
    "\n",
    "        \n",
    "    #validation\n",
    "    net.eval()\n",
    "    start=time.time()\n",
    "    with torch.no_grad():\n",
    "        eval_psnr=0\n",
    "        eval_ssim=0\n",
    "#         tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "        for idx_img, (lr, hr,date,_) in enumerate(val_dataloders):\n",
    "            print(\"start first batch validation\")\n",
    "\n",
    "            lr, hr = prepare([lr, hr])\n",
    "#             sr = net(lr, 0)\n",
    "#             val_loss=criterion(sr, hr)\n",
    "#             for ssr,hhr in zip(sr,hr):\n",
    "#                 eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "#                 eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() ) \n",
    "            print(\"end first batch validation\")\n",
    "\n",
    "    print(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "              e,\n",
    "              time.time()-start,\n",
    "              optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "              running_loss.item()/len(train_data),\n",
    "              val_loss\n",
    "         ))\n",
    "        \n",
    "    if running_loss<max_error:\n",
    "        print(\"saving\")\n",
    "        max_error=running_loss\n",
    "        if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "            os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "        f = open(\"./model/save/\"+args.train_name+\"/\"+str(e)+\".txt\",'w')\n",
    "        f.close()\n",
    "#         torch.save(net,\"./model/save/\"+args.train_name+\"/\"+str(e)+\".pkl\")\n",
    "        print(\"end Training for %d\"%e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(3):\n",
    "    if not os.path.exists(\"./model/save/\"+\"teamp1\"+\"/\"):\n",
    "        os.mkdir(\"./model/save/\"+\"teamp1\"+\"/\")\n",
    "    f = open(\"./model/save/\"+\"teamp1\"+\"/\"+str(i)+\".txt\",'w')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
