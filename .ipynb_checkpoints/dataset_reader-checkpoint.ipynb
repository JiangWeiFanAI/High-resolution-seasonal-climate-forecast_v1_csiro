{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "import argparse\n",
    "\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "import xarray as xr\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "file_BARRA_dir=\"/g/data/ma05/BARRA_R/analysis/acum_proc\"\n",
    "ensemble=['e01','e02']\n",
    "# ensemble=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "\n",
    "leading_time=217\n",
    "leading_time_we_use=31\n",
    "\n",
    "\n",
    "init_date=date(1970, 1, 1)\n",
    "start_date=date(1990, 1, 1)\n",
    "end_date=date(1990,12,31) #if 929 is true we should substract 1 day\n",
    "dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='BARRA_R and ACCESS-S!')\n",
    "def set_parser():\n",
    "    parser.add_argument('--debug', action='store_true',\n",
    "                        help='Enables debug mode')\n",
    "    parser.add_argument('--template', default='.',\n",
    "                        help='You can set various templates in option.py')\n",
    "\n",
    "    # Hardware specifications\n",
    "    parser.add_argument('--n_threads', type=int, default=3,\n",
    "                        help='number of threads for data loading')\n",
    "    parser.add_argument('--cpu', action='store_true',\n",
    "                        help='use cpu only')\n",
    "    parser.add_argument('--n_GPUs', type=int, default=2,\n",
    "                        help='number of GPUs')\n",
    "    parser.add_argument('--seed', type=int, default=1,\n",
    "                        help='random seed')\n",
    "\n",
    "    # Data specifications\n",
    "    parser.add_argument('--file_ACCESS_dir', type=str, \n",
    "                        default=\"F:/climate/access-s1/pr/daily/\",\n",
    "    \n",
    "                        help='dataset directory')\n",
    "    parser.add_argument('--file_BARRA_dir', type=str, \n",
    "                        default=\"C:/Users/JIA059/barra/\",\n",
    "                        help='dataset directory')\n",
    "    parser.add_argument('--nine2nine', type=bool, \n",
    "                        default=True,\n",
    "                        help='whether rainfall acculate from 9am to 9am')\n",
    "    parser.add_argument('--date_minus_one', type=int, \n",
    "                        default=1,\n",
    "                        help='whether rainfall acculate from yesterday(1)/today(0) 9am to tody/tomorrow 9am')\n",
    "    \n",
    "    \n",
    "    parser.add_argument('--dir_demo', type=str, default='../test',\n",
    "                        help='demo image directory')\n",
    "#     parser.add_argument('--data_train', type=str, default='BARRA_R',\n",
    "#                         help='train dataset name')\n",
    "#     parser.add_argument('--data_test', type=str, default='DIV2K',\n",
    "#                         help='test dataset name')\n",
    "    parser.add_argument('--benchmark_noise', action='store_true',\n",
    "                        help='use noisy benchmark sets')\n",
    "    parser.add_argument('--n_train', type=int, default=800,\n",
    "                        help='number of training set')\n",
    "    parser.add_argument('--n_val', type=int, default=10,\n",
    "                        help='number of validation set')\n",
    "    parser.add_argument('--offset_val', type=int, default=800,\n",
    "                        help='validation index offest')\n",
    "    parser.add_argument('--ext', type=str, default='sep',\n",
    "                        help='dataset file extension')\n",
    "    parser.add_argument('--scale', default='4',\n",
    "                        help='super resolution scale')\n",
    "    parser.add_argument('--patch_size', type=int, default=96,\n",
    "                        help='output patch size')\n",
    "    #??????????????????????????????????????????????????\n",
    "    parser.add_argument('--rgb_range', type=int, default=300,\n",
    "                        help='maximum value of RGB')\n",
    "    parser.add_argument('--n_colors', type=int, default=1,\n",
    "                        help='number of color channels to use')\n",
    "    parser.add_argument('--noise', type=str, default='.',\n",
    "                        help='Gaussian noise std.')\n",
    "    parser.add_argument('--chop', action='store_true',\n",
    "                        help='enable memory-efficient forward')\n",
    "\n",
    "    # Model specifications\n",
    "    parser.add_argument('--model', default='RCAN',\n",
    "                        help='model name')\n",
    "\n",
    "    parser.add_argument('--act', type=str, default='relu',\n",
    "                        help='activation function')\n",
    "    parser.add_argument('--pre_train', type=str, default='.',\n",
    "                        help='pre-trained model directory')\n",
    "    parser.add_argument('--extend', type=str, default='.',\n",
    "                        help='pre-trained model directory')\n",
    "    parser.add_argument('--n_resblocks', type=int, default=16,\n",
    "                        help='number of residual blocks')\n",
    "    parser.add_argument('--n_feats', type=int, default=64,\n",
    "                        help='number of feature maps')\n",
    "    parser.add_argument('--res_scale', type=float, default=1,\n",
    "                        help='residual scaling')\n",
    "    parser.add_argument('--shift_mean', default=True,\n",
    "                        help='subtract pixel mean from the input')\n",
    "    parser.add_argument('--precision', type=str, default='single',\n",
    "                        choices=('single', 'half'),\n",
    "                        help='FP precision for test (single | half)')\n",
    "\n",
    "    # Training specifications\n",
    "    parser.add_argument('--reset', action='store_true',\n",
    "                        help='reset the training')\n",
    "    parser.add_argument('--test_every', type=int, default=1000,\n",
    "                        help='do test per every N batches')\n",
    "    parser.add_argument('--epochs', type=int, default=3000,\n",
    "                        help='number of epochs to train')\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "                        help='input batch size for training')\n",
    "    parser.add_argument('--split_batch', type=int, default=1,\n",
    "                        help='split the batch into smaller chunks')\n",
    "    parser.add_argument('--self_ensemble', action='store_true',\n",
    "                        help='use self-ensemble method for test')\n",
    "    parser.add_argument('--test_only', action='store_true',\n",
    "                        help='set this option to test the model')\n",
    "    parser.add_argument('--gan_k', type=int, default=1,\n",
    "                        help='k value for adversarial loss')\n",
    "\n",
    "    # Optimization specifications\n",
    "    parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--lr_decay', type=int, default=200,\n",
    "                        help='learning rate decay per N epochs')\n",
    "    parser.add_argument('--decay_type', type=str, default='step',\n",
    "                        help='learning rate decay type')\n",
    "    parser.add_argument('--gamma', type=float, default=0.5,\n",
    "                        help='learning rate decay factor for step decay')\n",
    "    parser.add_argument('--optimizer', default='ADAM',\n",
    "                        choices=('SGD', 'ADAM', 'RMSprop'),\n",
    "                        help='optimizer to use (SGD | ADAM | RMSprop)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='SGD momentum')\n",
    "    parser.add_argument('--beta1', type=float, default=0.9,\n",
    "                        help='ADAM beta1')\n",
    "    parser.add_argument('--beta2', type=float, default=0.999,\n",
    "                        help='ADAM beta2')\n",
    "    parser.add_argument('--epsilon', type=float, default=1e-8,\n",
    "                        help='ADAM epsilon for numerical stability')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                        help='weight decay')\n",
    "\n",
    "    # Loss specifications\n",
    "    parser.add_argument('--loss', type=str, default='1*L1',\n",
    "                        help='loss function configuration')\n",
    "    parser.add_argument('--skip_threshold', type=float, default='1e6',\n",
    "                        help='skipping batch that has large error')\n",
    "\n",
    "    # Log specifications\n",
    "    parser.add_argument('--save', type=str, default='RCAN',\n",
    "                        help='file name to save')\n",
    "    parser.add_argument('--load', type=str, default='.',\n",
    "                        help='file name to load')\n",
    "    parser.add_argument('--resume', type=int, default=0,\n",
    "                        help='resume from specific checkpoint')\n",
    "    parser.add_argument('--print_model', action='store_true',\n",
    "                        help='print model')\n",
    "    parser.add_argument('--save_models', action='store_true',\n",
    "                        help='save all intermediate models')\n",
    "    parser.add_argument('--print_every', type=int, default=100,\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save_results', action='store_true',\n",
    "                        help='save output results')\n",
    "\n",
    "    # New options\n",
    "    parser.add_argument('--n_resgroups', type=int, default=10,\n",
    "                        help='number of residual groups')\n",
    "    parser.add_argument('--reduction', type=int, default=16,\n",
    "                        help='number of feature maps reduction')\n",
    "    parser.add_argument('--testpath', type=str, default='../test/DIV2K_val_LR_our',\n",
    "                        help='dataset directory for testing')\n",
    "    parser.add_argument('--testset', type=str, default='Set5',\n",
    "                        help='dataset name for testing')\n",
    "    parser.add_argument('--degradation', type=str, default='BI',\n",
    "                        help='degradation model: BI, BD')\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    # args = parser.parse_args()\n",
    "#     template.set_template(args)\n",
    "\n",
    "    args.scale = list(map(lambda x: int(x), args.scale.split('+')))\n",
    "    \n",
    "    if args.epochs == 0:\n",
    "        args.epochs = 1e8\n",
    "\n",
    "    for arg in vars(args):\n",
    "        if vars(args)[arg] == 'True':\n",
    "            vars(args)[arg] = True\n",
    "        elif vars(args)[arg] == 'False':\n",
    "            vars(args)[arg] = False\n",
    "    return args\n",
    "\n",
    "args=set_parser()\n",
    "# args.template.find(\"DDBPN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2789\n",
      "[112.08333, 156.25, -44.166664, -10.277771]\n",
      "((62, 54), (308, 402))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# domain = [111.975, 156.275, -44.525, -9.975]\n",
    "\n",
    "file_ACCESS_dir=\"F:/climate/access-s1/pr/daily/\"#\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "file_BARRA_dir=\"F:/climate/barra/\"\n",
    "\n",
    "class ACCESS_BARRA_v1(Dataset):\n",
    "    '''\n",
    "    scale is size(hr)=size(lr)*scale\n",
    "    version_1_documention: the data we use is raw data that store at NCI\n",
    "    '''\n",
    "    def __init__(self,start_date=date(1990, 1, 1),end_date=date(1990,12 , 31),regin=\"AUS\",transform=None,args=args):\n",
    "        self.file_BARRA_dir = args.file_BARRA_dir\n",
    "        self.file_ACCESS_dir = args.file_ACCESS_dir\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "        self.scale = args.scale[0]\n",
    "        self.regin = regin\n",
    "        \n",
    "        if regin==\"AUS\":\n",
    "            self.shape=(314,403,1,1)\n",
    "            self.domain=[111.975, 156.275, -44.525, -9.975]\n",
    "        else:\n",
    "            self.shape=(768,1200,1,1)\n",
    "                \n",
    "        self.dates = self.date_range(start_date, end_date)\n",
    "        \n",
    "        \n",
    "        self.filename_list=self.get_filename_with_time_order(args.file_ACCESS_dir)\n",
    "\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filename_list)\n",
    "    \n",
    "\n",
    "    def date_range(self,start_date, end_date):\n",
    "        \"\"\"This function takes a start date and an end date as datetime date objects.\n",
    "        It returns a list of dates for each date in order starting at the first date and ending with the last date\"\"\"\n",
    "        return [start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    \n",
    "    def get_filename_with_no_time_order(self,rootdir):\n",
    "        '''get filename first and generate label '''\n",
    "        _files = []\n",
    "        list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "        for i in range(0,len(list)):\n",
    "            path = os.path.join(rootdir,list[i])\n",
    "            if os.path.isdir(path):\n",
    "                _files.extend(self.get_filename_with_no_time_order(path))\n",
    "            if os.path.isfile(path):\n",
    "                if path[-3:]==\".nc\":\n",
    "                    _files.append(path)\n",
    "        return _files\n",
    "    \n",
    "    def get_filename_with_time_order(self,rootdir):\n",
    "        '''get filename first and generate label '''\n",
    "        _files = []\n",
    "        for en in ensemble:\n",
    "            for date in dates:\n",
    "                filename=\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "                access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "#                 print(access_path)\n",
    "                if os.path.exists(access_path):\n",
    "                    for i in range(leading_time_we_use):\n",
    "                        path=[access_path]\n",
    "                        \n",
    "#                         barra_path=file_BARRA_dir+\"/accum_prcp-an-spec-PT0H-BARRA_R-v1-\"+((date+timedelta(i)).strftime(\"%Y%m%d\"))\n",
    "                        barra_date=date+timedelta(i)\n",
    "#                         self.data_dir+date.strftime('%m')+\"/accum_prcp-an-spec-PT0H-BARRA_R-v1-\"\\\n",
    "#                         +date.strftime('%Y%m%d')+\"T\"+enum[i]+\"Z.nc\"\n",
    "                        path.append(barra_date)\n",
    "                        path.append(i)\n",
    "#                         print(path)\n",
    "                        _files.append(path)\n",
    "    \n",
    "    #最后去掉第一行，然后shuffle\n",
    "        if args.nine2nine and args.date_minus_one==1:\n",
    "            del _files[0]\n",
    "        return _files\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        from filename idx get id\n",
    "        return lr,hr\n",
    "        '''\n",
    "        #read_data filemame[idx]\n",
    "        access_filename,date_for_BARRA,time_leading=self.filename_list[idx]\n",
    "#         print(type(date_for_BARRA))\n",
    "#         low_filename,high_filename,time_leading=self.filename_list[idx]\n",
    "\n",
    "        data_low=dpt.read_access_data(access_filename,idx=idx)\n",
    "        train_data=dpt.map_aust(data_low)\n",
    "        \n",
    "        domain = [train_data.lon.data.min(), train_data.lon.data.max(), train_data.lat.data.min(), train_data.lat.data.max()]\n",
    "        print(domain)\n",
    "\n",
    "        data_high=dpt.read_barra_data_fc(self.file_BARRA_dir,date_for_BARRA,nine2nine=False)\n",
    "        label=dpt.map_aust(data_high,domain=domain)\n",
    "        \n",
    "#         print(train_data.shape,label.shape)\n",
    "#         print(train_data.shape[0]*4,train_data.shape[1]*4)\n",
    "#         print(label.shape[0]/4,label.shape[1]/4)\n",
    "\n",
    "        return train_data.shape,label.shape\n",
    "\n",
    "    \n",
    "\n",
    "# ACCESS_BARRA(file_access_dir,file_BARRA_dir).filename_list\n",
    "data_set=ACCESS_BARRA_v1(args=args)\n",
    "print(len(data_set))\n",
    "# for i in data_set.filename_list:\n",
    "#     print(i)\n",
    "print(data_set[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
