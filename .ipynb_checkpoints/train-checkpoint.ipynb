{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5114674452354135\n",
      "training statistics:\n",
      "  ------------------------------\n",
      "  trainning name  |  temp01\n",
      "  ------------------------------\n",
      "  num of channels |     1\n",
      "  ------------------------------\n",
      "  num of threads  |     8\n",
      "  ------------------------------\n",
      "  batch_size      |    16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v1,ACCESS_BARRA_v2\n",
    "\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import model\n",
    "import utility\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xarray as xr\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "\n",
    "args.file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "args.file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "# training_name=\"temp01\"\n",
    "args.file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "args.channels=0\n",
    "if args.pr:\n",
    "    args.channels+=1\n",
    "if args.zg:\n",
    "    args.channels+=1\n",
    "if args.psl:\n",
    "    args.channels+=1\n",
    "if args.tasmax:\n",
    "    args.channels+=1\n",
    "if args.tasmin:\n",
    "    args.channels+=1\n",
    "access_rgb_mean= 2.9067910245780248e-05*86400\n",
    "\n",
    "leading_time=217\n",
    "args.leading_time_we_use=7\n",
    "args.ensemble=2\n",
    "\n",
    "init_date=date(1970, 1, 1)\n",
    "start_date=date(1990, 1, 2)\n",
    "# end_date=date(2012,12,25) #if 929 is true we should substract 1 day\n",
    "dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "print(access_rgb_mean)\n",
    "\n",
    "print(\"training statistics:\")\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  trainning name  |  %s\"%args.train_name)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  num of channels | %5d\"%args.channels)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  batch_size      | %5d\"%args.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5114674452354135\n",
      "training statistics:\n",
      "  ------------------------------\n",
      "  trainning name  |  temp01\n",
      "  ------------------------------\n",
      "  num of channels |     1\n",
      "  ------------------------------\n",
      "  num of threads  |     8\n",
      "  ------------------------------\n",
      "  batch_size      |    16\n"
     ]
    }
   ],
   "source": [
    "#for debuging on my computer\n",
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v1,ACCESS_BARRA_v2\n",
    "\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import model\n",
    "import utility\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xarray as xr\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "\n",
    "args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "# args.file_ACCESS_dir=\"H:/climate/access-s1/\"\n",
    "\n",
    "args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "# args.file_BARRA_dir=\"D:/dataset/accum_prcp/\"\n",
    "\n",
    "args.channels=1\n",
    "args.batch_size=16\n",
    "\n",
    "# ensemble=['e01','e02']\n",
    "args.ensemble=2\n",
    "access_rgb_mean= 2.9067910245780248e-05*86400\n",
    "\n",
    "leading_time=217\n",
    "args.leading_time_we_use=7\n",
    "# args.lr=0.001\n",
    "\n",
    "init_date=date(1970, 1, 1)\n",
    "start_date=date(1990, 1, 2)\n",
    "end_date=date(1990,12,25) #if 929 is true we should substract 1 day\n",
    "dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "print(access_rgb_mean)\n",
    "print(\"training statistics:\")\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  trainning name  |  %s\"%args.train_name)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  num of channels | %5d\"%args.channels)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  batch_size      | %5d\"%args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> BARRA_R & ACCESS_S1 loading\n",
      "=> from 1990/01/02 to 1990/12/25\n",
      "C:/Users/JIA059/barra/\n",
      "no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Dataset statistics:\n",
      "  ------------------------------\n",
      "  total |   603\n",
      "  ------------------------------\n",
      "  train |   482\n",
      "  ------------------------------\n",
      "  val   |   121\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "#     transforms.Resize(IMG_SIZE),\n",
    "#     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(30),\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "])\n",
    "\n",
    "data_set=ACCESS_BARRA_v2(start_date,end_date,transform=train_transforms,args=args)\n",
    "train_data,val_data=random_split(data_set,[int(len(data_set)*0.8),len(data_set)-int(len(data_set)*0.8)])\n",
    "\n",
    "\n",
    "print(\"Dataset statistics:\")\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  total | %5d\"%len(data_set))\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  train | %5d\"%len(train_data))\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  val   | %5d\"%len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataloders =DataLoader(train_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=False,\n",
    "                            num_workers=0)\n",
    "val_dataloders =DataLoader(val_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=False,\n",
    "                          num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making model...\n",
      "accesss-s1 mean (0.4690)\n"
     ]
    }
   ],
   "source": [
    "args.cpu=True\n",
    "# args.pre_train =False\n",
    "# args.pre_train =\"C:/Users/JIA059/climate_v1_csiro/High-resolution-seasonal-climate-forecast_v1_csiro/model/RCAN_BIX\"+str(args.scale[0])+\".pt\"\n",
    "# \"C:/Users/JIA059/climate_v1_csiro/High-resolution-seasonal-climate-forecast_v1_csiro/model\"\n",
    "def prepare( l, volatile=False):\n",
    "    device = torch.device('cpu' if args.cpu else 'cuda')\n",
    "    def _prepare(tensor):\n",
    "        if args.precision == 'half': tensor = tensor.half()\n",
    "        return tensor.to(device)\n",
    "\n",
    "    return [_prepare(_l) for _l in l]\n",
    "\n",
    "checkpoint = utility.checkpoint(args)\n",
    "net = model.Model(args, checkpoint).double()\n",
    "args.lr=0.001\n",
    "criterion = nn.L1Loss()\n",
    "optimizer_my = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer_my, step_size=7, gamma=0.1)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer_my, gamma=0.9)\n",
    "# torch.optim.lr_scheduler.MultiStepLR(optimizer_my, milestones=[20,80], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(args.lr)\n",
    "# %matplotlib inline\n",
    "# lr_list = []\n",
    "# LR = 0.01\n",
    "# epoch=800\n",
    "# for epoch in range(100):\n",
    "#     scheduler.step()\n",
    "#     lr_list.append(optimizer_my.state_dict()['param_groups'][0]['lr'])\n",
    "# plt.plot(range(100),lr_list,color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end loading one data,time cost 2.676250\n",
      "end loading one data,time cost 3.161077\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7d0a6ae2ef24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mrunning_loss\u001b[0m \u001b[1;31m#.copy()?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mrunning_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0moptimizer_my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training\n",
    "\n",
    "\n",
    "max_error=np.inf\n",
    "for e in range(args.epochs):\n",
    "    #train\n",
    "    net.train()\n",
    "    loss=0\n",
    "    start=time.time()\n",
    "#     if e % 10 == 0:\n",
    "#         for p in optimizer_my.param_groups:\n",
    "#             p['lr'] *= 0.9\n",
    "\n",
    "    for batch, (lr, hr,_,_) in enumerate(train_dataloders):\n",
    "    #     print(batch, (lr.size(), hr.size()))\n",
    "        lr, hr = prepare([lr, hr])\n",
    "        optimizer_my.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            sr = net(lr, 0)\n",
    "#         error = criterion(sr[:,:,:,0:403], hr)\n",
    "            running_loss =criterion(sr, hr)\n",
    "            loss+=running_loss #.copy()?\n",
    "        running_loss.backward()\n",
    "        optimizer_my.step()\n",
    "        \n",
    "    #validation\n",
    "    net.eval()\n",
    "    start=time.time()\n",
    "    with torch.no_grad():\n",
    "        eval_psnr=0\n",
    "        eval_ssim=0\n",
    "        tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "        for idx_img, (lr, hr,date,_) in enumerate(tqdm_val):\n",
    "            lr, hr = prepare([lr, hr])\n",
    "            sr = net(lr, 0)\n",
    "            val_loss=criterion(sr, hr)\n",
    "            for ssr,hhr in zip(sr,hr):\n",
    "                eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "                eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )      \n",
    "#             print(\"psnr: %f \"%(eval_psnr/len(test_data)))\n",
    "    print(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "              e,\n",
    "              time.time()-start,\n",
    "              optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "              loss.item()/len(train_data),\n",
    "              val_loss\n",
    "         ))\n",
    "    if running_loss<max_error:\n",
    "        max_error=running_loss\n",
    "#         torch.save(net,train_loss\"_\"+str(e)+\".pkl\")\n",
    "        if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "            os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "        torch.save(net,\"./model/save/\"+args.train_name+\"/\"+str(e)+\".pkl\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.train_name=\"temp02\"\n",
    "if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "    os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "# torch.save(net,\"./model/save/\"+training_name+\"/\"+str(e)+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing,and evaluation\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net.eval()\n",
    "start=time.time()\n",
    "with torch.no_grad():\n",
    "    eval_psnr=0\n",
    "    eval_ssim=0\n",
    "    tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "    for idx_img, (lr, hr,date,_) in enumerate(tqdm_val):\n",
    "        lr, hr = prepare([lr, hr])\n",
    "        sr = net(lr, 0)\n",
    "        val_loss=criterion(sr, hr)\n",
    "        for ssr,hhr in zip(sr,hr):\n",
    "            eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "            eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "            \n",
    "            \n",
    "            \n",
    "        break\n",
    "#     break\n",
    "            \n",
    "    print(\"psnr: %f \"%(eval_psnr/len(val_data)))\n",
    "        \n",
    "print(\"time cost: %f s \"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (len(test_dataloders)*args.batch_size )\n",
    "print(0+val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hr.max()-hr.min()).item()\n",
    "print(\"psnr: %f \"%(hr.max()-hr.min()).item() )\n",
    "ssim_avg=0\n",
    "psnr_avg=0\n",
    "for ssr,hhr in zip(sr,hr):\n",
    "#     print(hhr[0].shape)\n",
    "#     print(compare_ssim(ssr[0].numpy(),hhr[0].numpy(),data_range=(hhr[0].max()-hhr[0].min()).item() ))\n",
    "    psnr_avg+=compare_psnr(ssr[0].numpy(),hhr[0].numpy(),data_range=(hhr[0].max()-hhr[0].min()).item() )\n",
    "    ssim_avg+=compare_ssim(ssr[0].numpy(),hhr[0].numpy(),data_range=(hhr[0].max()-hhr[0].min()).item() )\n",
    "print(ssim_avg/args.batch_size)\n",
    "print(psnr_avg/args.batch_size)\n",
    "compare_psnr(ssr[0].numpy(),hhr[0].numpy(),data_range=(hhr[0].max()-hhr[0].min()).item() )\n",
    "\n",
    "print(args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sr.shape,hr.shape)\n",
    "compare_psnr(hhr[0].numpy(),hhr[0].numpy(),data_range=(hhr[0].max()-hhr[0].min()).item() )\n",
    "print(len(test_data))\n",
    "print(len(test_dataloders))\n",
    "# data_set.lon.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo:\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    import time\n",
    "    start=time.time()\n",
    "    print(\"test time for sigle day %s \" %(time.time()-start))\n",
    "\n",
    "    sr_numpy=sr.numpy()[7][0]\n",
    "#     sr_numpy=np.ascontiguousarray(sr_numpy.transpose((1, 0)))\n",
    "    data_sr=xr.DataArray(sr_numpy,coords=[data_set.lat.data,data_set.lon.data],dims=[\"lat\",\"lon\"])\n",
    "    \n",
    "    hr_numpy=hr.numpy()[7][0]\n",
    "#     hr_numpy=np.ascontiguousarray(hr_numpy.transpose((1, 0)))\n",
    "    data_hr=xr.DataArray(hr_numpy,coords=[data_set.lat.data,data_set.lon.data],dims=[\"lat\",\"lon\"])\n",
    "    print(date)\n",
    "    dpt.draw_aus(data_hr,\n",
    "                 colormap = plt.cm.get_cmap('RdBu_r', 10),\n",
    "                 title=\"super resolution we predicted\",\n",
    "                 save=False)\n",
    "    dpt.draw_aus(data_sr,\n",
    "                 colormap = plt.cm.get_cmap('RdBu_r', 10),\n",
    "                 title=\"super resolution we predicted\",\n",
    "                 save=False)\n",
    "\n",
    "# psnr1(sr, hr)\n",
    "# print(hr.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hr_numpy.shape)\n",
    "print(data_set.lon.data.shape)\n",
    "print(data_set.lat.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "\n",
    "\n",
    "\n",
    "epoch = self.scheduler.last_epoch + 1\n",
    "self.ckp.write_log('\\nEvaluation:')\n",
    "self.ckp.add_log(torch.zeros(1, len(self.scale)))\n",
    "self.model.eval()\n",
    "\n",
    "timer_test = utility.timer()\n",
    "with torch.no_grad():\n",
    "    for idx_scale, scale in enumerate(self.scale):\n",
    "        eval_acc = 0\n",
    "        self.loader_test.dataset.set_scale(idx_scale)\n",
    "        tqdm_test = tqdm(self.loader_test, ncols=80)\n",
    "        for idx_img, (lr, hr, filename, _) in enumerate(tqdm_test):\n",
    "            filename = filename[0]\n",
    "            no_eval = (hr.nelement() == 1)\n",
    "            if not no_eval:\n",
    "                lr, hr = self.prepare([lr, hr])\n",
    "            else:\n",
    "                lr = self.prepare([lr])[0]\n",
    "\n",
    "            sr = self.model(lr, idx_scale)\n",
    "            sr = utility.quantize(sr, self.args.rgb_range)\n",
    "\n",
    "            save_list = [sr]\n",
    "            if not no_eval:\n",
    "                eval_acc += utility.calc_psnr(\n",
    "                    sr, hr, scale, self.args.rgb_range,\n",
    "                    benchmark=self.loader_test.dataset.benchmark\n",
    "                )\n",
    "                save_list.extend([lr, hr])\n",
    "\n",
    "            if self.args.save_results:\n",
    "                self.ckp.save_results(filename, save_list, scale)\n",
    "\n",
    "        self.ckp.log[-1, idx_scale] = eval_acc / len(self.loader_test)\n",
    "        best = self.ckp.log.max(0)\n",
    "        self.ckp.write_log(\n",
    "            '[{} x{}]\\tPSNR: {:.3f} (Best: {:.3f} @epoch {})'.format(\n",
    "                self.args.data_test,\n",
    "                scale,\n",
    "                self.ckp.log[-1, idx_scale],\n",
    "                best[0][idx_scale],\n",
    "                best[1][idx_scale] + 1\n",
    "            )\n",
    "        )\n",
    "\n",
    "self.ckp.write_log(\n",
    "    'Total time: {:.2f}s\\n'.format(timer_test.toc()), refresh=True\n",
    ")\n",
    "if not self.args.test_only:\n",
    "    self.ckp.save(self, epoch, is_best=(best[1][0] + 1 == epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.new()\n",
    "np.random.rand(8, 4, 78, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "\n",
    "\n",
    "# channel=1\n",
    "# x=np.random.rand(8,channel, 78, 100)\n",
    "# x=x.astype(np.float32)\n",
    "# print(x.dtype)\n",
    "\n",
    "# means, stdevs = [], []\n",
    "\n",
    "# for i in range(channel):\n",
    "#     pixels = x[:, i, :, :].ravel()  # 拉成一行\n",
    "#     means.append(np.mean(pixels))\n",
    "#     stdevs.append(np.std(pixels))\n",
    "\n",
    "# x=torch.tensor(x)\n",
    "\n",
    "\n",
    "# # std=np.std(x.ravel(),axis=0)\n",
    "# # print(std.shape)\n",
    "# # mean=np.mean(x)\n",
    "# print(means)\n",
    "# print(stdevs)\n",
    "\n",
    "# # x=torch.rand((8, 3, 78, 100))\n",
    "# # std=torch.std(x)\n",
    "# # mean=torch.mean(x)\n",
    "\n",
    "# # x.dtype=\"float32\"\n",
    "\n",
    "# class MeanShift(nn.Conv2d):\n",
    "#     def __init__(self, rgb_range, rgb_mean, rgb_std,channels, sign=-1):\n",
    "#         super(MeanShift, self).__init__(channel, channel, kernel_size=1)\n",
    "#         std = torch.Tensor(rgb_std)\n",
    "#         self.weight.data = torch.eye(channel).view(channel, channel, 1, 1)\n",
    "#         self.weight.data.div_(std.view(channel, 1, 1, 1))\n",
    "#         self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n",
    "#         self.bias.data.div_(std)\n",
    "#         self.requires_grad = False\n",
    "        \n",
    "# aa=MeanShift(1,means,stdevs,3)\n",
    "# # aa(x)\n",
    "# np.mean(aa(x).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checkpoint = utility.checkpoint(args)\n",
    "# # net = model.Model(args, checkpoint)\n",
    "# # criterion = nn.L1Loss()\n",
    "# # optimizer_my = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)\n",
    "# def prepare( l, volatile=False):\n",
    "#     device = torch.device('cpu' if args.cpu else 'cuda')\n",
    "#     def _prepare(tensor):\n",
    "#         if args.precision == 'half': tensor = tensor.half()\n",
    "#         return tensor.to(device)\n",
    "\n",
    "#     return [_prepare(_l) for _l in l]\n",
    "\n",
    "# max_error=10000\n",
    "# for e in range(args.epochs):\n",
    "#     if e % 10 == 0:\n",
    "#         for p in optimizer_my.param_groups:\n",
    "#             p['lr'] *= 0.9\n",
    "        \n",
    "#     net.train()\n",
    "#     for batch, (lr, hr) in enumerate(train_dataloders):\n",
    "#         lr, hr = prepare([lr, hr])\n",
    "# #         print(lr)\n",
    "\n",
    "#         optimizer_my.zero_grad()\n",
    "#         sr = net(lr, 0)\n",
    "# #         sr=utility.fit_size(sr,args)\n",
    "\n",
    "#         error = criterion(sr[:,:,:,0:403], hr)\n",
    "#         if error<max_error:\n",
    "#             max_error=error\n",
    "#             torch.save(net,\"C:/Users/JIA059/climate_v1_csiro/High-resolution-seasonal-climate-forecast_v1_csiro/model/save/\"+str(e)+\".pkl\")\n",
    "#         error.backward()\n",
    "#         optimizer_my.step()\n",
    "#     print(\"epoche: %d, lr: %f, error: %f\"%(e,optimizer_my.state_dict()['param_groups'][0]['lr'],error.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
