{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='BARRA_R and ACCESS-S!')\n",
    "parser.add_argument('--args_test', type=int, default=0,\n",
    "                        help='testing parameters input')\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "                    help='Enables debug mode')\n",
    "parser.add_argument('--template', default='.',\n",
    "                    help='You can set various templates in option.py')\n",
    "\n",
    "# Hardware specifications\n",
    "parser.add_argument('--n_threads', type=int, default=0,\n",
    "                    help='number of threads for data loading')\n",
    "parser.add_argument('--cpu', action='store_true',\n",
    "                    help='use cpu only')\n",
    "parser.add_argument('--n_GPUs', type=int, default=1,\n",
    "                    help='number of GPUs')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed')\n",
    "\n",
    "# Data specifications\n",
    "parser.add_argument('--pr', type=bool, \n",
    "                default=True,\n",
    "                help='add-on pr?')\n",
    "\n",
    "parser.add_argument('--dem', type=bool, \n",
    "                default=False,\n",
    "                help='add-on dem?') \n",
    "parser.add_argument('--psl', type=bool, \n",
    "                default=False,\n",
    "                help='add-on psl?') \n",
    "parser.add_argument('--zg', type=bool, \n",
    "                default=False,\n",
    "                help='add-on zg?') \n",
    "parser.add_argument('--tasmax', type=bool, \n",
    "                default=False,\n",
    "                help='add-on tasmax?') \n",
    "parser.add_argument('--tasmin', type=bool, \n",
    "                default=False,\n",
    "                help='add-on tasmin?')\n",
    "\n",
    "parser.add_argument('--leading_time_we_use', type=int, \n",
    "                default=7,\n",
    "                help='add-on tasmin?')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--ensemble', type=int, \n",
    "                default=2,\n",
    "                help='total ensambles is 11') \n",
    "\n",
    "\n",
    "parser.add_argument('--channels', type=float, \n",
    "                    default=0,\n",
    "                    help='channel of data_input must') \n",
    "#[111.85, 155.875, -44.35, -9.975]\n",
    "parser.add_argument('--domain', type=list, \n",
    "                    default=[112.9, 154.25, -43.7425, -9.0],\n",
    "                    help='dataset directory')    \n",
    "\n",
    "\n",
    "parser.add_argument('--file_ACCESS_dir', type=str, \n",
    "                    default=\"F:/climate/access-s1/pr/daily/\",\n",
    "\n",
    "                    help='dataset directory')\n",
    "parser.add_argument('--file_BARRA_dir', type=str, \n",
    "                    default=\"C:/Users/JIA059/barra/\",\n",
    "                    help='dataset directory')\n",
    "\n",
    "parser.add_argument('--file_DEM_dir', type=str, \n",
    "                    default=\"../DEM/\",\n",
    "                    help='dataset directory')\n",
    "\n",
    "parser.add_argument('--nine2nine', type=bool, \n",
    "                    default=True,\n",
    "                    help='whether rainfall acculate from 9am to 9am')\n",
    "parser.add_argument('--date_minus_one', type=int, \n",
    "                    default=1,\n",
    "                    help='whether rainfall acculate from yesterday(1)/today(0) 9am to tody/tomorrow 9am')\n",
    "\n",
    "\n",
    "parser.add_argument('--dir_demo', type=str, default='../test',\n",
    "                    help='demo image directory')\n",
    "#     parser.add_argument('--data_train', type=str, default='BARRA_R',\n",
    "#                         help='train dataset name')\n",
    "#     parser.add_argument('--data_test', type=str, default='DIV2K',\n",
    "#                         help='test dataset name')\n",
    "parser.add_argument('--benchmark_noise', action='store_true',\n",
    "                    help='use noisy benchmark sets')\n",
    "parser.add_argument('--n_train', type=int, default=800,\n",
    "                    help='number of training set')\n",
    "parser.add_argument('--n_val', type=int, default=10,\n",
    "                    help='number of validation set')\n",
    "parser.add_argument('--offset_val', type=int, default=800,\n",
    "                    help='validation index offest')\n",
    "parser.add_argument('--ext', type=str, default='sep',\n",
    "                    help='dataset file extension')\n",
    "parser.add_argument('--scale', default='4',\n",
    "                    help='super resolution scale')\n",
    "parser.add_argument('--patch_size', type=int, default=192,\n",
    "                    help='output patch size')\n",
    "#??????????????????????????????????????????????????\n",
    "parser.add_argument('--rgb_range', type=int, default=400,\n",
    "                    help='maximum value of RGB')\n",
    "parser.add_argument('--n_colors', type=int, default=3,\n",
    "                    help='number of color channels to use')\n",
    "parser.add_argument('--noise', type=str, default='.',\n",
    "                    help='Gaussian noise std.')\n",
    "parser.add_argument('--chop', action='store_true',\n",
    "                    help='enable memory-efficient forward')\n",
    "\n",
    "# Model specifications\n",
    "parser.add_argument('--model', default='RCAN',\n",
    "                    help='model name')\n",
    "\n",
    "parser.add_argument('--act', type=str, default='relu',\n",
    "                    help='activation function')\n",
    "parser.add_argument('--pre_train', type=str, default='.',\n",
    "                    help='pre-trained model directory')\n",
    "parser.add_argument('--extend', type=str, default='.',\n",
    "                    help='pre-trained model directory')\n",
    "parser.add_argument('--n_resblocks', type=int, default=20,\n",
    "                    help='number of residual blocks')\n",
    "parser.add_argument('--n_feats', type=int, default=64,\n",
    "                    help='number of feature maps')\n",
    "parser.add_argument('--res_scale', type=float, default=1,\n",
    "                    help='residual scaling')\n",
    "parser.add_argument('--shift_mean', default=True,\n",
    "                    help='subtract pixel mean from the input')\n",
    "parser.add_argument('--precision', type=str, default='single',\n",
    "                    choices=('single', 'half','double'),\n",
    "                    help='FP precision for test (single | half)')\n",
    "\n",
    "# Training specifications\n",
    "\n",
    "parser.add_argument('--train_name', type=str, default='temp01',\n",
    "                    help='the trainning name of the set')\n",
    "parser.add_argument('--reset', action='store_true',\n",
    "                    help='reset the training')\n",
    "parser.add_argument('--test_every', type=int, default=1000,\n",
    "                    help='do test per every N batches')\n",
    "parser.add_argument('--epochs', type=int, default=300,\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--batch_size', type=int, default=2,\n",
    "                    help='input batch size for training')\n",
    "parser.add_argument('--split_batch', type=int, default=1,\n",
    "                    help='split the batch into smaller chunks')\n",
    "parser.add_argument('--self_ensemble', action='store_true',\n",
    "                    help='use self-ensemble method for test')\n",
    "parser.add_argument('--test_only', action='store_true',\n",
    "                    help='set this option to test the model')\n",
    "parser.add_argument('--gan_k', type=int, default=1,\n",
    "                    help='k value for adversarial loss')\n",
    "\n",
    "# Optimization specifications\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--lr_decay', type=int, default=200,\n",
    "                    help='learning rate decay per N epochs')\n",
    "parser.add_argument('--decay_type', type=str, default='step',\n",
    "                    help='learning rate decay type')\n",
    "parser.add_argument('--gamma', type=float, default=0.5,\n",
    "                    help='learning rate decay factor for step decay')\n",
    "parser.add_argument('--optimizer', default='ADAM',\n",
    "                    choices=('SGD', 'ADAM', 'RMSprop'),\n",
    "                    help='optimizer to use (SGD | ADAM | RMSprop)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='SGD momentum')\n",
    "parser.add_argument('--beta1', type=float, default=0.9,\n",
    "                    help='ADAM beta1')\n",
    "parser.add_argument('--beta2', type=float, default=0.999,\n",
    "                    help='ADAM beta2')\n",
    "parser.add_argument('--epsilon', type=float, default=1e-8,\n",
    "                    help='ADAM epsilon for numerical stability')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='weight decay')\n",
    "\n",
    "# Loss specifications\n",
    "parser.add_argument('--loss', type=str, default='1*L1',\n",
    "                    help='loss function configuration')\n",
    "parser.add_argument('--skip_threshold', type=float, default='1e6',\n",
    "                    help='skipping batch that has large error')\n",
    "\n",
    "# Log specifications\n",
    "parser.add_argument('--save', type=str, default='RCAN',\n",
    "                    help='file name to save')\n",
    "parser.add_argument('--load', type=str, default='.',\n",
    "                    help='file name to load')\n",
    "parser.add_argument('--resume', type=int, default=0,\n",
    "                    help='resume from specific checkpoint')\n",
    "parser.add_argument('--print_model', action='store_true',\n",
    "                    help='print model')\n",
    "parser.add_argument('--save_models', action='store_true',\n",
    "                    help='save all intermediate models')\n",
    "parser.add_argument('--print_every', type=int, default=100,\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save_results', action='store_true',\n",
    "                    help='save output results')\n",
    "\n",
    "# New options\n",
    "parser.add_argument('--n_resgroups', type=int, default=10,\n",
    "                    help='number of residual groups')\n",
    "parser.add_argument('--reduction', type=int, default=16,\n",
    "                    help='number of feature maps reduction')\n",
    "parser.add_argument('--testpath', type=str, default='../test/DIV2K_val_LR_our',\n",
    "                    help='dataset directory for testing')\n",
    "parser.add_argument('--testset', type=str, default='Set5',\n",
    "                    help='dataset name for testing')\n",
    "parser.add_argument('--degradation', type=str, default='BI',\n",
    "                    help='degradation model: BI, BD')\n",
    "# args = []\n",
    "# args = parser.parse_known_args()[0]\n",
    "import platform \n",
    "sys = platform.system()\n",
    "\n",
    "if sys == \"Windows\":\n",
    "    args = parser.parse_args(args=[])\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "#     template.set_template(args)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args.scale = list(map(lambda x: int(x), args.scale.split('+')))\n",
    "\n",
    "if args.epochs == 0:\n",
    "    args.epochs = 1e8\n",
    "\n",
    "for arg in vars(args):\n",
    "    if vars(args)[arg] == 'True':\n",
    "        vars(args)[arg] = True\n",
    "    elif vars(args)[arg] == 'False':\n",
    "        vars(args)[arg] = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "# from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v4\n",
    "import torch\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import model\n",
    "from model import my_model\n",
    "import utility\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xarray as xr\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "import platform\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def write_log(log):\n",
    "    print(log)\n",
    "    my_log_file=open(\"./model/save/\"+args.train_name + '/train.txt', 'a')\n",
    "#     log=\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time())\n",
    "    my_log_file.write(log + '\\n')\n",
    "    my_log_file.close()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "pre_train_path=\"./model/save/temp01/last.pth\"\n",
    "#     pre_train_path=\"./model/save/temp01/\"+0+\".pth\"\n",
    "\n",
    "\n",
    "\n",
    "init_date=date(1970, 1, 1)\n",
    "start_date=date(1990, 1, 2)\n",
    "# end_date=date(1990,12,25)\n",
    "end_date=date(2012,12,25) #if 929 is true we should substract 1 day    \n",
    "sys = platform.system()\n",
    "\n",
    "if sys == \"Windows\":\n",
    "    init_date=date(1970, 1, 1)\n",
    "    start_date=date(1990, 1, 2)\n",
    "    end_date=date(1990,12,15) #if 929 is true we should substract 1 day   \n",
    "    args.file_ACCESS_dir=\"H:/climate/access-s1/\" \n",
    "    args.file_BARRA_dir=\"D:/dataset/accum_prcp/\"\n",
    "#         args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "#         args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "    args.file_DEM_dir=\"../DEM/\"\n",
    "else:\n",
    "    args.file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "    args.file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "    # training_name=\"temp01\"\n",
    "    args.file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "args.dem=1\n",
    "args.channels=0\n",
    "if args.pr:\n",
    "    args.channels+=1\n",
    "if args.zg:\n",
    "    args.channels+=1\n",
    "if args.psl:\n",
    "    args.channels+=1\n",
    "if args.tasmax:\n",
    "    args.channels+=1\n",
    "if args.tasmin:\n",
    "    args.channels+=1\n",
    "if args.dem:\n",
    "    args.channels+=1\n",
    "access_rgb_mean= 2.9067910245780248e-05*86400\n",
    "\n",
    "leading_time=217\n",
    "args.leading_time_we_use=1\n",
    "args.ensemble=1\n",
    "\n",
    "\n",
    "print(access_rgb_mean)\n",
    "\n",
    "print(\"training statistics:\")\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  trainning name  |  %s\"%args.train_name)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  num of channels | %5d\"%args.channels)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  using cpu only？ | %5d\"%args.cpu)\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "#     transforms.Resize(IMG_SIZE),\n",
    "#     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(30),\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "])\n",
    "\n",
    "data_set=ACCESS_BARRA_v4(start_date,end_date,transform=train_transforms,args=args)\n",
    "train_data,val_data=random_split(data_set,[int(len(data_set)*0.8),len(data_set)-int(len(data_set)*0.8)])\n",
    "\n",
    "\n",
    "print(\"Dataset statistics:\")\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  total | %5d\"%len(data_set))\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  train | %5d\"%len(train_data))\n",
    "print(\"  ------------------------------\")\n",
    "print(\"  val   | %5d\"%len(val_data))\n",
    "\n",
    "###################################################################################set a the dataLoader\n",
    "train_dataloders =DataLoader(train_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=False,\n",
    "                            num_workers=args.n_threads)\n",
    "val_dataloders =DataLoader(val_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=False,\n",
    "                          num_workers=args.n_threads)\n",
    "##\n",
    "def prepare( l, volatile=False):\n",
    "    def _prepare(tensor):\n",
    "        if args.precision == 'half': tensor = tensor.half()\n",
    "        if args.precision == 'single': tensor = tensor.float()\n",
    "        return tensor.to(device)\n",
    "\n",
    "    return [_prepare(_l) for _l in l]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = utility.checkpoint(args)\n",
    "net = model.Model(args, checkpoint)\n",
    "#     net.load(\"./model/RCAN_BIX4.pt\", pre_train=\"./model/RCAN_BIX4.pt\", resume=args.resume, cpu=True)\n",
    "my_net=my_model.Modify_RCAN(net,args,checkpoint)\n",
    "\n",
    "#     net.load(\"./model/RCAN_BIX4.pt\", pre_train=\"./model/RCAN_BIX4.pt\", resume=args.resume, cpu=args.cpu)\n",
    "\n",
    "args.lr=0.000001\n",
    "criterion = nn.L1Loss()\n",
    "optimizer_my = optim.SGD(my_net.parameters(), lr=args.lr, momentum=0.9)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer_my, step_size=7, gamma=0.1)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer_my, gamma=0.9)\n",
    "# torch.optim.lr_scheduler.MultiStepLR(optimizer_my, milestones=[20,80], gamma=0.1)\n",
    "\n",
    "if args.resume==1:\n",
    "    print(\"continue last train\")\n",
    "    model_checkpoint = torch.load(pre_train_path)\n",
    "else:\n",
    "    print(\"restart train\")\n",
    "    model_checkpoint = torch.load(\"./model/save/temp01/first_\"+str(args.channels)+\".pth\")\n",
    "\n",
    "# my_net.load_state_dict(model_checkpoint['model'])\n",
    "# optimizer_my.load_state_dict(model_checkpoint['optimizer'])\n",
    "# epoch = model_checkpoint['epoch']\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    my_net = nn.DataParallel(my_net)\n",
    "else:\n",
    "    write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "\n",
    "#     my_net = torch.nn.DataParallel(my_net)\n",
    "my_net.to(device)\n",
    "\n",
    "##########################################################################training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.channels==1:\n",
    "    write_log(\"start\")\n",
    "    max_error=np.inf\n",
    "    for e in range(args.epochs):\n",
    "        #train\n",
    "        my_net.train()\n",
    "        loss=0\n",
    "        start=time.time()\n",
    "        for batch, (pr,hr,_,_) in enumerate(train_dataloders):\n",
    "            write_log(\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time()))\n",
    "            \n",
    "#             start=time.time()\n",
    "            pr,hr= prepare([pr,hr])\n",
    "            optimizer_my.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                sr = my_net(pr)\n",
    "                print(pr.shape)\n",
    "                print(sr.shape)\n",
    "\n",
    "                running_loss =criterion(sr, hr)\n",
    "\n",
    "                running_loss.backward()\n",
    "                optimizer_my.step()\n",
    "            loss+=running_loss #.copy()?\n",
    "            if batch%10==0:\n",
    "                state = {'model': my_net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "                torch.save(state, \"./model/save/temp01/last.pth\")\n",
    "            write_log(\"Train done,train time cost %f s,learning rate:%f, loss: %f\"%(start-time.time(),optimizer_my.state_dict()['param_groups'][0]['lr'] ,running_loss.item()  ))\n",
    "            start=time.time()\n",
    "\n",
    "        #validation\n",
    "        my_net.eval()\n",
    "        start=time.time()\n",
    "        with torch.no_grad():\n",
    "            eval_psnr=0\n",
    "            eval_ssim=0\n",
    "#             tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "            for batch, (pr,dem,hr,_,_) in enumerate(val_dataloders):\n",
    "                pr,dem,hr = prepare([pr,dem,hr])\n",
    "                sr = my_net(pr,dem)\n",
    "                val_loss=criterion(sr, hr)\n",
    "                for ssr,hhr in zip(sr,hr):\n",
    "                    eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "                    eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "\n",
    "        write_log(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "                  e,\n",
    "                  time.time()-start,\n",
    "                  optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "                  loss.item()/len(train_data),\n",
    "                  val_loss\n",
    "             ))\n",
    "#         print(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "#                   e,\n",
    "#                   time.time()-start,\n",
    "#                   optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "#                   loss.item()/len(train_data),\n",
    "#                   val_loss\n",
    "#              ))\n",
    "        if running_loss<max_error:\n",
    "            max_error=running_loss\n",
    "    #         torch.save(net,train_loss\"_\"+str(e)+\".pkl\")\n",
    "            if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "                os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "            write_log(\"saving\")\n",
    "            state = {'model': my_net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "            torch.save(state, \"./model/save/temp01/\"+str(e)+\".pth\")\n",
    "#             torch.save(net,\"./model/save/\"+args.train_name+\"/\"+str(e)+\".pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if args.channels==2:\n",
    "    write_log(\"start\")\n",
    "    max_error=np.inf\n",
    "    for e in range(args.epochs):\n",
    "        #train\n",
    "        my_net.train()\n",
    "        loss=0\n",
    "        start=time.time()\n",
    "        for batch, (pr,dem,hr,_,_) in enumerate(train_dataloders):\n",
    "            write_log(\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time()))\n",
    "            \n",
    "#             start=time.time()\n",
    "            pr,dem,hr= prepare([pr,dem,hr])\n",
    "            optimizer_my.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                sr = my_net(pr,dem)\n",
    "                print(pr.shape)\n",
    "                print(sr.shape)\n",
    "\n",
    "                running_loss =criterion(sr, hr)\n",
    "\n",
    "                running_loss.backward()\n",
    "                optimizer_my.step()\n",
    "            loss+=running_loss #.copy()?\n",
    "            if batch%10==0:\n",
    "                state = {'model': my_net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "                torch.save(state, \"./model/save/temp01/last.pth\")\n",
    "            write_log(\"Train done,train time cost %f s,learning rate:%f, loss: %f\"%(start-time.time(),optimizer_my.state_dict()['param_groups'][0]['lr'] ,running_loss.item()  ))\n",
    "            start=time.time()\n",
    "\n",
    "        #validation\n",
    "        my_net.eval()\n",
    "        start=time.time()\n",
    "        with torch.no_grad():\n",
    "            eval_psnr=0\n",
    "            eval_ssim=0\n",
    "#             tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "            for batch, (pr,dem,hr,_,_) in enumerate(val_dataloders):\n",
    "                pr,dem,hr = prepare([pr,dem,hr])\n",
    "                sr = my_net(pr,dem)\n",
    "                val_loss=criterion(sr, hr)\n",
    "                for ssr,hhr in zip(sr,hr):\n",
    "                    eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "                    eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "\n",
    "        write_log(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "                  e,\n",
    "                  time.time()-start,\n",
    "                  optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "                  loss.item()/len(train_data),\n",
    "                  val_loss\n",
    "             ))\n",
    "#         print(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "#                   e,\n",
    "#                   time.time()-start,\n",
    "#                   optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "#                   loss.item()/len(train_data),\n",
    "#                   val_loss\n",
    "#              ))\n",
    "        if running_loss<max_error:\n",
    "            max_error=running_loss\n",
    "    #         torch.save(net,train_loss\"_\"+str(e)+\".pkl\")\n",
    "            if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "                os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "            write_log(\"saving\")\n",
    "            state = {'model': my_net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "            torch.save(state, \"./model/save/temp01/\"+str(e)+\".pth\")\n",
    "#             torch.save(net,\"./model/save/\"+args.train_name+\"/\"+str(e)+\".pkl\")\n",
    "\n",
    "else:\n",
    "    write_log(\"start\")\n",
    "    max_error=np.inf\n",
    "    for e in range(args.epochs):\n",
    "        #train\n",
    "        my_net.train()\n",
    "        loss=0\n",
    "        start=time.time()\n",
    "        for batch, (pr,dem,psl,zg,tasmax,tasmin, hr,_,_) in enumerate(train_dataloders):\n",
    "            write_log(\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time()))\n",
    "            start=time.time()\n",
    "            pr,dem,psl,zg,tasmax,tasmin, hr = prepare([pr,dem,psl,zg,tasmax,tasmin, hr])\n",
    "            print(sr)\n",
    "            optimizer_my.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                sr = my_net(pr,dem,psl,zg,tasmax,tasmin)\n",
    "                running_loss =criterion(sr, hr)\n",
    "\n",
    "                running_loss.backward()\n",
    "                optimizer_my.step()\n",
    "            loss+=running_loss #.copy()?\n",
    "            if batch%10==0:\n",
    "                state = {'model': my_net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "                torch.save(state, \"./model/save/temp01/last.pth\")\n",
    "            write_log(\"Train done,train time cost %f s,learning rate:%f, loss: %f\"%(start-time.time(),optimizer_my.state_dict()['param_groups'][0]['lr'] ,running_loss.item()  ))\n",
    "            start=time.time()\n",
    "\n",
    "        #validation\n",
    "        my_net.eval()\n",
    "        start=time.time()\n",
    "        with torch.no_grad():\n",
    "            eval_psnr=0\n",
    "            eval_ssim=0\n",
    "#             tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "            for idx_img, (pr,dem,psl,zg,tasmax,tasmin, hr,_,_) in enumerate(val_dataloders):\n",
    "                pr,dem,psl,zg,tasmax,tasmin, hr = prepare([pr,dem,psl,zg,tasmax,tasmin, hr])\n",
    "                sr = my_net(pr,dem,psl,zg,tasmax,tasmin)\n",
    "                val_loss=criterion(sr, hr)\n",
    "                for ssr,hhr in zip(sr,hr):\n",
    "                    eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "                    eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "\n",
    "        write_log(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "                  e,\n",
    "                  time.time()-start,\n",
    "                  optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "                  loss.item()/len(train_data),\n",
    "                  val_loss\n",
    "             ))\n",
    "#         print(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "#                   e,\n",
    "#                   time.time()-start,\n",
    "#                   optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "#                   loss.item()/len(train_data),\n",
    "#                   val_loss\n",
    "#              ))\n",
    "        if running_loss<max_error:\n",
    "            max_error=running_loss\n",
    "    #         torch.save(net,train_loss\"_\"+str(e)+\".pkl\")\n",
    "            if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "                os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "            write_log(\"saving\")\n",
    "            state = {'model': my_net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "            torch.save(state, \"./model/save/temp01/\"+str(e)+\".pth\")\n",
    "#             torch.save(net,\"./model/save/\"+args.train_name+\"/\"+str(e)+\".pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    " with torch.no_grad():\n",
    "        asd=my_net(pr,dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataloders =DataLoader(train_data,\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=0)\n",
    "val_dataloders =DataLoader(val_data,\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False,\n",
    "                           num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "args.cpu=True\n",
    "# args.pre_train =False\n",
    "# args.pre_train =\"C:/Users/JIA059/climate_v1_csiro/High-resolution-seasonal-climate-forecast_v1_csiro/model/RCAN_BIX\"+str(args.scale[0])+\".pt\"\n",
    "# \"C:/Users/JIA059/climate_v1_csiro/High-resolution-seasonal-climate-forecast_v1_csiro/model\"\n",
    "def prepare( l, volatile=False):\n",
    "    device = torch.device('cpu' if args.cpu else 'cuda')\n",
    "    def _prepare(tensor):\n",
    "        if args.precision == 'half': tensor = tensor.half()\n",
    "        return tensor.to(device)\n",
    "\n",
    "    return [_prepare(_l) for _l in l]\n",
    "\n",
    "checkpoint = utility.checkpoint(args)\n",
    "net = model.Model(args, checkpoint).double()\n",
    "args.lr=0.001\n",
    "criterion = nn.L1Loss()\n",
    "optimizer_my = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer_my, step_size=7, gamma=0.1)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer_my, gamma=0.9)\n",
    "# torch.optim.lr_scheduler.MultiStepLR(optimizer_my, milestones=[20,80], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib import import_module\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args, ckp):\n",
    "        super(Model, self).__init__()\n",
    "        print('Making model...')\n",
    "\n",
    "        self.scale = args.scale\n",
    "        self.idx_scale = 0\n",
    "        self.self_ensemble = args.self_ensemble\n",
    "        self.chop = args.chop\n",
    "        self.precision = args.precision\n",
    "        self.cpu = args.cpu\n",
    "        self.device = torch.device('cpu' if args.cpu else 'cuda')\n",
    "        self.n_GPUs = args.n_GPUs\n",
    "        self.save_models = args.save_models\n",
    "\n",
    "        module = import_module('model.' + args.model.lower())\n",
    "        self.model = module.make_model(args).to(self.device)\n",
    "        if args.precision == 'half': self.model.half()\n",
    "\n",
    "        if not args.cpu and args.n_GPUs > 1:\n",
    "            ckp.my_write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model, range(args.n_GPUs))\n",
    "\n",
    "        self.load(\n",
    "            ckp.dir,\n",
    "            pre_train=args.pre_train,\n",
    "            resume=args.resume,\n",
    "            cpu=args.cpu\n",
    "        )\n",
    "        if args.print_model: print(self.model)\n",
    "\n",
    "    def forward(self, x, idx_scale):\n",
    "        self.idx_scale = idx_scale\n",
    "        target = self.get_model()\n",
    "        if hasattr(target, 'set_scale'):\n",
    "            target.set_scale(idx_scale)\n",
    "\n",
    "        if self.self_ensemble and not self.training:\n",
    "            if self.chop:\n",
    "                forward_function = self.forward_chop\n",
    "            else:\n",
    "                forward_function = self.model.forward\n",
    "\n",
    "            return self.forward_x8(x, forward_function)\n",
    "        elif self.chop and not self.training:\n",
    "            return self.forward_chop(x)\n",
    "        else:\n",
    "            return self.model(x)\n",
    "\n",
    "    def get_model(self):\n",
    "        if self.n_GPUs == 1:\n",
    "            return self.model\n",
    "        else:\n",
    "            return self.model\n",
    "            return self.model.module\n",
    "\n",
    "    def state_dict(self, **kwargs):\n",
    "        target = self.get_model()\n",
    "        return target.state_dict(**kwargs)\n",
    "\n",
    "    def save(self, apath, epoch, is_best=False):\n",
    "        target = self.get_model()\n",
    "        torch.save(\n",
    "            target.state_dict(), \n",
    "            os.path.join(apath, 'model', 'model_latest.pt')\n",
    "        )\n",
    "        if is_best:\n",
    "            torch.save(\n",
    "                target.state_dict(),\n",
    "                os.path.join(apath, 'model', 'model_best.pt')\n",
    "            )\n",
    "        else:\n",
    "            torch.save(\n",
    "                target.state_dict(),\n",
    "                os.path.join(apath, 'model', 'model_{}.pt'.format(epoch))\n",
    "            )\n",
    "        \n",
    "        if self.save_models:\n",
    "            torch.save(\n",
    "                target.state_dict(),\n",
    "                os.path.join(apath, 'model', 'model_{}.pt'.format(epoch))\n",
    "            )\n",
    "\n",
    "    def load(self, apath, pre_train='.', resume=-1, cpu=False):\n",
    "        if cpu:\n",
    "            kwargs = {'map_location': lambda storage, loc: storage}\n",
    "        else:\n",
    "            kwargs = {}\n",
    "\n",
    "        if resume == -1:\n",
    "            self.get_model().load_state_dict(\n",
    "                torch.load(\n",
    "                    os.path.join(apath, 'model', 'model_latest.pt'),\n",
    "                    **kwargs\n",
    "                ),\n",
    "                strict=False\n",
    "            )\n",
    "        elif resume == 0:\n",
    "            if pre_train != '.':\n",
    "                print('Loading model from {}'.format(pre_train))\n",
    "                self.get_model().load_state_dict(\n",
    "                    torch.load(pre_train, **kwargs),\n",
    "                    strict=False\n",
    "                )\n",
    "        else:\n",
    "            self.get_model().load_state_dict(\n",
    "                torch.load(\n",
    "                    os.path.join(apath, 'model', 'model_{}.pt'.format(resume)),\n",
    "                    **kwargs\n",
    "                ),\n",
    "                strict=False\n",
    "            )\n",
    "    # shave = 10, min_size=160000\n",
    "    def forward_chop(self, x, shave=10, min_size=160000):\n",
    "        scale = self.scale[self.idx_scale]\n",
    "        n_GPUs = min(self.n_GPUs, 4)\n",
    "        b, c, h, w = x.size()\n",
    "        h_half, w_half = h // 2, w // 2\n",
    "        h_size, w_size = h_half + shave, w_half + shave\n",
    "        lr_list = [\n",
    "            x[:, :, 0:h_size, 0:w_size],\n",
    "            x[:, :, 0:h_size, (w - w_size):w],\n",
    "            x[:, :, (h - h_size):h, 0:w_size],\n",
    "            x[:, :, (h - h_size):h, (w - w_size):w]]\n",
    "\n",
    "        if w_size * h_size < min_size:\n",
    "            sr_list = []\n",
    "            for i in range(0, 4, n_GPUs):\n",
    "                lr_batch = torch.cat(lr_list[i:(i + n_GPUs)], dim=0)\n",
    "                sr_batch = self.model(lr_batch)\n",
    "                sr_list.extend(sr_batch.chunk(n_GPUs, dim=0))\n",
    "        else:\n",
    "            sr_list = [\n",
    "                self.forward_chop(patch, shave=shave, min_size=min_size) \\\n",
    "                for patch in lr_list\n",
    "            ]\n",
    "\n",
    "        h, w = scale * h, scale * w\n",
    "        h_half, w_half = scale * h_half, scale * w_half\n",
    "        h_size, w_size = scale * h_size, scale * w_size\n",
    "        shave *= scale\n",
    "\n",
    "        output = x.new(b, c, h, w)\n",
    "        output[:, :, 0:h_half, 0:w_half] \\\n",
    "            = sr_list[0][:, :, 0:h_half, 0:w_half]\n",
    "        output[:, :, 0:h_half, w_half:w] \\\n",
    "            = sr_list[1][:, :, 0:h_half, (w_size - w + w_half):w_size]\n",
    "        output[:, :, h_half:h, 0:w_half] \\\n",
    "            = sr_list[2][:, :, (h_size - h + h_half):h_size, 0:w_half]\n",
    "        output[:, :, h_half:h, w_half:w] \\\n",
    "            = sr_list[3][:, :, (h_size - h + h_half):h_size, (w_size - w + w_half):w_size]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward_x8(self, x, forward_function):\n",
    "        def _transform(v, op):\n",
    "            if self.precision != 'single': v = v.float()\n",
    "\n",
    "            v2np = v.data.cpu().numpy()\n",
    "            if op == 'v':\n",
    "                tfnp = v2np[:, :, :, ::-1].copy()\n",
    "            elif op == 'h':\n",
    "                tfnp = v2np[:, :, ::-1, :].copy()\n",
    "            elif op == 't':\n",
    "                tfnp = v2np.transpose((0, 1, 3, 2)).copy()\n",
    "\n",
    "            ret = torch.Tensor(tfnp).to(self.device)\n",
    "            if self.precision == 'half': ret = ret.half()\n",
    "\n",
    "            return ret\n",
    "\n",
    "        lr_list = [x]\n",
    "        for tf in 'v', 'h', 't':\n",
    "            lr_list.extend([_transform(t, tf) for t in lr_list])\n",
    "\n",
    "        sr_list = [forward_function(aug) for aug in lr_list]\n",
    "        for i in range(len(sr_list)):\n",
    "            if i > 3:\n",
    "                sr_list[i] = _transform(sr_list[i], 't')\n",
    "            if i % 4 > 1:\n",
    "                sr_list[i] = _transform(sr_list[i], 'h')\n",
    "            if (i % 4) % 2 == 1:\n",
    "                sr_list[i] = _transform(sr_list[i], 'v')\n",
    "\n",
    "        output_cat = torch.cat(sr_list, dim=0)\n",
    "        output = output_cat.mean(dim=0, keepdim=True)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "\n",
    "max_error=np.inf\n",
    "for e in range(args.epochs):\n",
    "    #train\n",
    "    net.train()\n",
    "    loss=0\n",
    "    start=time.time()\n",
    "#     if e % 10 == 0:\n",
    "#         for p in optimizer_my.param_groups:\n",
    "#             p['lr'] *= 0.9\n",
    "\n",
    "    for batch, (lr, hr,_,_) in enumerate(train_dataloders):\n",
    "    #     print(batch, (lr.size(), hr.size()))\n",
    "        lr, hr = prepare([lr, hr])\n",
    "        optimizer_my.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            sr = net(lr, 0)\n",
    "#         error = criterion(sr[:,:,:,0:403], hr)\n",
    "            running_loss =criterion(sr, hr)\n",
    "            loss+=running_loss #.copy()?\n",
    "        running_loss.backward()\n",
    "        optimizer_my.step()\n",
    "        \n",
    "    #validation\n",
    "    net.eval()\n",
    "    start=time.time()\n",
    "    with torch.no_grad():\n",
    "        eval_psnr=0\n",
    "        eval_ssim=0\n",
    "        tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "        for idx_img, (lr, hr,date,_) in enumerate(tqdm_val):\n",
    "            lr, hr = prepare([lr, hr])\n",
    "            sr = net(lr, 0)\n",
    "            val_loss=criterion(sr, hr)\n",
    "            for ssr,hhr in zip(sr,hr):\n",
    "                eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "                eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )      \n",
    "#             print(\"psnr: %f \"%(eval_psnr/len(test_data)))\n",
    "    print(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "              e,\n",
    "              time.time()-start,\n",
    "              optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "              loss.item()/len(train_data),\n",
    "              val_loss\n",
    "         ))\n",
    "    if running_loss<max_error:\n",
    "        max_error=running_loss\n",
    "#         torch.save(net,train_loss\"_\"+str(e)+\".pkl\")\n",
    "        if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "            os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "        torch.save(net,\"./model/save/\"+args.train_name+\"/\"+str(e)+\".pkl\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.train_name=\"temp02\"\n",
    "if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "    os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "# torch.save(net,\"./model/save/\"+training_name+\"/\"+str(e)+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing,and evaluation\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net.eval()\n",
    "start=time.time()\n",
    "with torch.no_grad():\n",
    "    eval_psnr=0\n",
    "    eval_ssim=0\n",
    "    tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "    for idx_img, (lr, hr,date,_) in enumerate(tqdm_val):\n",
    "        lr, hr = prepare([lr, hr])\n",
    "        sr = net(lr, 0)\n",
    "        val_loss=criterion(sr, hr)\n",
    "        for ssr,hhr in zip(sr,hr):\n",
    "            eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "            eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "            \n",
    "            \n",
    "            \n",
    "        break\n",
    "#     break\n",
    "            \n",
    "    print(\"psnr: %f \"%(eval_psnr/len(val_data)))\n",
    "        \n",
    "print(\"time cost: %f s \"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (len(test_dataloders)*args.batch_size )\n",
    "print(0+val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hr.max()-hr.min()).item()\n",
    "print(\"psnr: %f \"%(hr.max()-hr.min()).item() )\n",
    "ssim_avg=0\n",
    "psnr_avg=0\n",
    "for ssr,hhr in zip(sr,hr):\n",
    "#     print(hhr[0].shape)\n",
    "#     print(compare_ssim(ssr[0].numpy(),hhr[0].numpy(),data_range=(hhr[0].max()-hhr[0].min()).item() ))\n",
    "    psnr_avg+=compare_psnr(ssr[0].numpy(),hhr[0].numpy(),data_range=(hhr[0].max()-hhr[0].min()).item() )\n",
    "    ssim_avg+=compare_ssim(ssr[0].numpy(),hhr[0].numpy(),data_range=(hhr[0].max()-hhr[0].min()).item() )\n",
    "print(ssim_avg/args.batch_size)\n",
    "print(psnr_avg/args.batch_size)\n",
    "compare_psnr(ssr[0].numpy(),hhr[0].numpy(),data_range=(hhr[0].max()-hhr[0].min()).item() )\n",
    "\n",
    "print(args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sr.shape,hr.shape)\n",
    "compare_psnr(hhr[0].numpy(),hhr[0].numpy(),data_range=(hhr[0].max()-hhr[0].min()).item() )\n",
    "print(len(test_data))\n",
    "print(len(test_dataloders))\n",
    "# data_set.lon.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo:\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    import time\n",
    "    start=time.time()\n",
    "    print(\"test time for sigle day %s \" %(time.time()-start))\n",
    "\n",
    "    sr_numpy=sr.numpy()[7][0]\n",
    "#     sr_numpy=np.ascontiguousarray(sr_numpy.transpose((1, 0)))\n",
    "    data_sr=xr.DataArray(sr_numpy,coords=[data_set.lat.data,data_set.lon.data],dims=[\"lat\",\"lon\"])\n",
    "    \n",
    "    hr_numpy=hr.numpy()[7][0]\n",
    "#     hr_numpy=np.ascontiguousarray(hr_numpy.transpose((1, 0)))\n",
    "    data_hr=xr.DataArray(hr_numpy,coords=[data_set.lat.data,data_set.lon.data],dims=[\"lat\",\"lon\"])\n",
    "    print(date)\n",
    "    dpt.draw_aus(data_hr,\n",
    "                 colormap = plt.cm.get_cmap('RdBu_r', 10),\n",
    "                 title=\"super resolution we predicted\",\n",
    "                 save=False)\n",
    "    dpt.draw_aus(data_sr,\n",
    "                 colormap = plt.cm.get_cmap('RdBu_r', 10),\n",
    "                 title=\"super resolution we predicted\",\n",
    "                 save=False)\n",
    "\n",
    "# psnr1(sr, hr)\n",
    "# print(hr.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hr_numpy.shape)\n",
    "print(data_set.lon.data.shape)\n",
    "print(data_set.lat.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "\n",
    "\n",
    "\n",
    "epoch = self.scheduler.last_epoch + 1\n",
    "self.ckp.write_log('\\nEvaluation:')\n",
    "self.ckp.add_log(torch.zeros(1, len(self.scale)))\n",
    "self.model.eval()\n",
    "\n",
    "timer_test = utility.timer()\n",
    "with torch.no_grad():\n",
    "    for idx_scale, scale in enumerate(self.scale):\n",
    "        eval_acc = 0\n",
    "        self.loader_test.dataset.set_scale(idx_scale)\n",
    "        tqdm_test = tqdm(self.loader_test, ncols=80)\n",
    "        for idx_img, (lr, hr, filename, _) in enumerate(tqdm_test):\n",
    "            filename = filename[0]\n",
    "            no_eval = (hr.nelement() == 1)\n",
    "            if not no_eval:\n",
    "                lr, hr = self.prepare([lr, hr])\n",
    "            else:\n",
    "                lr = self.prepare([lr])[0]\n",
    "\n",
    "            sr = self.model(lr, idx_scale)\n",
    "            sr = utility.quantize(sr, self.args.rgb_range)\n",
    "\n",
    "            save_list = [sr]\n",
    "            if not no_eval:\n",
    "                eval_acc += utility.calc_psnr(\n",
    "                    sr, hr, scale, self.args.rgb_range,\n",
    "                    benchmark=self.loader_test.dataset.benchmark\n",
    "                )\n",
    "                save_list.extend([lr, hr])\n",
    "\n",
    "            if self.args.save_results:\n",
    "                self.ckp.save_results(filename, save_list, scale)\n",
    "\n",
    "        self.ckp.log[-1, idx_scale] = eval_acc / len(self.loader_test)\n",
    "        best = self.ckp.log.max(0)\n",
    "        self.ckp.write_log(\n",
    "            '[{} x{}]\\tPSNR: {:.3f} (Best: {:.3f} @epoch {})'.format(\n",
    "                self.args.data_test,\n",
    "                scale,\n",
    "                self.ckp.log[-1, idx_scale],\n",
    "                best[0][idx_scale],\n",
    "                best[1][idx_scale] + 1\n",
    "            )\n",
    "        )\n",
    "\n",
    "self.ckp.write_log(\n",
    "    'Total time: {:.2f}s\\n'.format(timer_test.toc()), refresh=True\n",
    ")\n",
    "if not self.args.test_only:\n",
    "    self.ckp.save(self, epoch, is_best=(best[1][0] + 1 == epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.new()\n",
    "np.random.rand(8, 4, 78, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "\n",
    "\n",
    "# channel=1\n",
    "# x=np.random.rand(8,channel, 78, 100)\n",
    "# x=x.astype(np.float32)\n",
    "# print(x.dtype)\n",
    "\n",
    "# means, stdevs = [], []\n",
    "\n",
    "# for i in range(channel):\n",
    "#     pixels = x[:, i, :, :].ravel()  # 拉成一行\n",
    "#     means.append(np.mean(pixels))\n",
    "#     stdevs.append(np.std(pixels))\n",
    "\n",
    "# x=torch.tensor(x)\n",
    "\n",
    "\n",
    "# # std=np.std(x.ravel(),axis=0)\n",
    "# # print(std.shape)\n",
    "# # mean=np.mean(x)\n",
    "# print(means)\n",
    "# print(stdevs)\n",
    "\n",
    "# # x=torch.rand((8, 3, 78, 100))\n",
    "# # std=torch.std(x)\n",
    "# # mean=torch.mean(x)\n",
    "\n",
    "# # x.dtype=\"float32\"\n",
    "\n",
    "# class MeanShift(nn.Conv2d):\n",
    "#     def __init__(self, rgb_range, rgb_mean, rgb_std,channels, sign=-1):\n",
    "#         super(MeanShift, self).__init__(channel, channel, kernel_size=1)\n",
    "#         std = torch.Tensor(rgb_std)\n",
    "#         self.weight.data = torch.eye(channel).view(channel, channel, 1, 1)\n",
    "#         self.weight.data.div_(std.view(channel, 1, 1, 1))\n",
    "#         self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n",
    "#         self.bias.data.div_(std)\n",
    "#         self.requires_grad = False\n",
    "        \n",
    "# aa=MeanShift(1,means,stdevs,3)\n",
    "# # aa(x)\n",
    "# np.mean(aa(x).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checkpoint = utility.checkpoint(args)\n",
    "# # net = model.Model(args, checkpoint)\n",
    "# # criterion = nn.L1Loss()\n",
    "# # optimizer_my = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)\n",
    "# def prepare( l, volatile=False):\n",
    "#     device = torch.device('cpu' if args.cpu else 'cuda')\n",
    "#     def _prepare(tensor):\n",
    "#         if args.precision == 'half': tensor = tensor.half()\n",
    "#         return tensor.to(device)\n",
    "\n",
    "#     return [_prepare(_l) for _l in l]\n",
    "\n",
    "# max_error=10000\n",
    "# for e in range(args.epochs):\n",
    "#     if e % 10 == 0:\n",
    "#         for p in optimizer_my.param_groups:\n",
    "#             p['lr'] *= 0.9\n",
    "        \n",
    "#     net.train()\n",
    "#     for batch, (lr, hr) in enumerate(train_dataloders):\n",
    "#         lr, hr = prepare([lr, hr])\n",
    "# #         print(lr)\n",
    "\n",
    "#         optimizer_my.zero_grad()\n",
    "#         sr = net(lr, 0)\n",
    "# #         sr=utility.fit_size(sr,args)\n",
    "\n",
    "#         error = criterion(sr[:,:,:,0:403], hr)\n",
    "#         if error<max_error:\n",
    "#             max_error=error\n",
    "#             torch.save(net,\"C:/Users/JIA059/climate_v1_csiro/High-resolution-seasonal-climate-forecast_v1_csiro/model/save/\"+str(e)+\".pkl\")\n",
    "#         error.backward()\n",
    "#         optimizer_my.step()\n",
    "#     print(\"epoche: %d, lr: %f, error: %f\"%(e,optimizer_my.state_dict()['param_groups'][0]['lr'],error.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end loading one data,time cost 1.715966\n",
    "end loading one data,time cost 1.632074\n",
    "end loading one data,time cost 1.671565\n",
    "end loading one data,time cost 1.715903\n",
    "end loading one data,time cost 1.605141\n",
    "end loading one data,time cost 1.621977\n",
    "end loading one data,time cost 1.621583\n",
    "end loading one data,time cost 1.621500\n",
    "end loading one data,time cost 1.600586\n",
    "end loading one data,time cost 1.594560\n",
    "end loading one data,time cost 1.593534\n",
    "end loading one data,time cost 1.604488\n",
    "end loading one data,time cost 1.606978\n",
    "end loading one data,time cost 1.600660\n",
    "end loading one data,time cost 1.600594\n",
    "end loading one data,time cost 1.605244\n",
    "end loading one data,time cost 1.692404\n",
    "end loading one data,time cost 1.619076\n",
    "end loading one data,time cost 1.682503\n",
    "end loading one data,time cost 1.620104\n",
    "end loading one data,time cost 1.628786\n",
    "end loading one data,time cost 1.607090\n",
    "end loading one data,time cost 1.670968\n",
    "end loading one data,time cost 1.671936\n",
    "end loading one data,time cost 1.602363\n",
    "end loading one data,time cost 1.654448\n",
    "end loading one data,time cost 1.673330\n",
    "end loading one data,time cost 1.623322\n",
    "end loading one data,time cost 1.635231\n",
    "end loading one data,time cost 1.642935\n",
    "end loading one data,time cost 1.650784\n",
    "end loading one data,time cost 1.645927\n",
    "end loading one data,time cost 1.726419\n",
    "end loading one data,time cost 1.690695\n",
    "end loading one data,time cost 1.689773\n",
    "end loading one data,time cost 1.711888\n",
    "end loading one data,time cost 1.612949\n",
    "end loading one data,time cost 1.617190\n",
    "end loading one data,time cost 1.640625\n",
    "end loading one data,time cost 1.674064\n",
    "end loading one data,time cost 1.642350\n",
    "end loading one data,time cost 1.647243\n",
    "end loading one data,time cost 1.606059\n",
    "end loading one data,time cost 1.593960\n",
    "end loading one data,time cost 1.629991\n",
    "end loading one data,time cost 1.602437\n",
    "end loading one data,time cost 1.647727\n",
    "end loading one data,time cost 1.660140\n",
    "end loading one data,time cost 1.712363\n",
    "end loading one data,time cost 1.706504\n",
    "end loading one data,time cost 1.684479\n",
    "end loading one data,time cost 1.705693\n",
    "end loading one data,time cost 1.656246\n",
    "end loading one data,time cost 1.632491\n",
    "end loading one data,time cost 1.641997\n",
    "end loading one data,time cost 1.640489\n",
    "end loading one data,time cost 1.615456\n",
    "end loading one data,time cost 1.657358\n",
    "end loading one data,time cost 1.619042\n",
    "end loading one data,time cost 1.591613\n",
    "end loading one data,time cost 1.642784\n",
    "end loading one data,time cost 1.649220\n",
    "end loading one data,time cost 1.614412\n",
    "end loading one data,time cost 1.629373\n",
    "end loading one data,time cost 1.681119\n",
    "end loading one data,time cost 1.661355\n",
    "end loading one data,time cost 1.656491\n",
    "end loading one data,time cost 1.663970\n",
    "end loading one data,time cost 1.660205\n",
    "end loading one data,time cost 1.646195\n",
    "end loading one data,time cost 1.666683\n",
    "end loading one data,time cost 1.668686\n",
    "end loading one data,time cost 1.625349\n",
    "end loading one data,time cost 1.636791\n",
    "end loading one data,time cost 1.644629\n",
    "end loading one data,time cost 1.653769\n",
    "end loading one data,time cost 1.667453\n",
    "end loading one data,time cost 1.653591\n",
    "end loading one data,time cost 1.660961\n",
    "end loading one data,time cost 1.613453\n",
    "end loading one data,time cost 1.696342\n",
    "end loading one data,time cost 1.649550\n",
    "end loading one data,time cost 1.633617\n",
    "end loading one data,time cost 1.665265\n",
    "end loading one data,time cost 1.657074\n",
    "end loading one data,time cost 1.632226\n",
    "end loading one data,time cost 1.695935\n",
    "end loading one data,time cost 1.654280\n",
    "end loading one data,time cost 1.635847\n",
    "end loading one data,time cost 1.608618\n",
    "end loading one data,time cost 1.642667\n",
    "end loading one data,time cost 1.651407\n",
    "end loading one data,time cost 1.668395\n",
    "end loading one data,time cost 1.671665\n",
    "end loading one data,time cost 1.657387\n",
    "end loading one data,time cost 1.668278\n",
    "end loading one data,time cost 1.723833\n",
    "end loading one data,time cost 1.661024\n",
    "end loading one data,time cost 1.662430\n",
    "end loading one data,time cost 1.684431\n",
    "end loading one data,time cost 1.645400\n",
    "end loading one data,time cost 1.617971\n",
    "end loading one data,time cost 1.611092\n",
    "end loading one data,time cost 1.622975\n",
    "end loading one data,time cost 1.649650\n",
    "end loading one data,time cost 1.641810\n",
    "end loading one data,time cost 1.656899\n",
    "end loading one data,time cost 1.656220\n",
    "end loading one data,time cost 1.665733\n",
    "end loading one data,time cost 1.688295\n",
    "end loading one data,time cost 1.658993\n",
    "end loading one data,time cost 1.674594\n",
    "end loading one data,time cost 1.703684\n",
    "end loading one data,time cost 1.704101\n",
    "end loading one data,time cost 1.638448\n",
    "end loading one data,time cost 1.674996\n",
    "end loading one data,time cost 1.631276\n",
    "end loading one data,time cost 1.634202\n",
    "end loading one data,time cost 1.636754\n",
    "end loading one data,time cost 1.702930\n",
    "end loading one data,time cost 1.654904\n",
    "end loading one data,time cost 1.643668\n",
    "end loading one data,time cost 1.663055\n",
    "end loading one data,time cost 1.641192\n",
    "end loading one data,time cost 1.652675\n",
    "end loading one data,time cost 1.675678\n",
    "end loading one data,time cost 1.642454\n",
    "end loading one data,time cost 1.672420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end loading one data,time cost 1.633667\n",
    "end loading one data,time cost 1.608869\n",
    "end loading one data,time cost 1.585573\n",
    "end loading one data,time cost 1.603048\n",
    "end loading one data,time cost 1.639884\n",
    "end loading one data,time cost 1.633841\n",
    "end loading one data,time cost 1.678963\n",
    "end loading one data,time cost 1.641207\n",
    "end loading one data,time cost 1.625344\n",
    "end loading one data,time cost 1.623207\n",
    "end loading one data,time cost 1.649238\n",
    "end loading one data,time cost 1.635630\n",
    "end loading one data,time cost 1.604664\n",
    "end loading one data,time cost 1.610379\n",
    "end loading one data,time cost 1.652588\n",
    "end loading one data,time cost 1.597581\n",
    "end loading one data,time cost 1.583224\n",
    "end loading one data,time cost 1.629541\n",
    "end loading one data,time cost 1.627871\n",
    "end loading one data,time cost 1.657890\n",
    "end loading one data,time cost 1.611370\n",
    "end loading one data,time cost 1.586311\n",
    "end loading one data,time cost 1.673285\n",
    "end loading one data,time cost 1.651267\n",
    "end loading one data,time cost 1.655988\n",
    "end loading one data,time cost 1.631040\n",
    "end loading one data,time cost 1.618624\n",
    "end loading one data,time cost 1.654610\n",
    "end loading one data,time cost 1.605169\n",
    "end loading one data,time cost 1.590393\n",
    "end loading one data,time cost 1.681857\n",
    "end loading one data,time cost 1.626186\n",
    "end loading one data,time cost 1.657457\n",
    "end loading one data,time cost 1.642736\n",
    "end loading one data,time cost 1.670448\n",
    "end loading one data,time cost 1.646338\n",
    "end loading one data,time cost 1.601079\n",
    "end loading one data,time cost 1.588476\n",
    "end loading one data,time cost 1.643233\n",
    "end loading one data,time cost 1.688879\n",
    "end loading one data,time cost 1.599864\n",
    "end loading one data,time cost 1.585036\n",
    "end loading one data,time cost 1.619789\n",
    "end loading one data,time cost 1.589529\n",
    "end loading one data,time cost 1.587214\n",
    "end loading one data,time cost 1.607371"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
